{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a390d671",
   "metadata": {},
   "source": [
    "# T5 model for text simplification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279090df",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23440e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade torch\n",
    "!pip install SentencePiece\n",
    "!pip uninstall transformers -y\n",
    "!pip install transformers\n",
    "!pip install rich\n",
    "!pip install rouge\n",
    "!pip install evaluate\n",
    "!pip install Levenshtein\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bc01186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "from rich.table import Column, Table\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from string import punctuation\n",
    "import Levenshtein\n",
    "import spacy\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Lock\n",
    "import threading\n",
    "from queue import Queue\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55cc1f1",
   "metadata": {},
   "source": [
    "## Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32838c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, data_path='train/simpletext_task3_train.json', qrels_path='train/simpletext_task3_qrels.json'):\n",
    "        self.data_path = data_path\n",
    "        self.qrels_path = qrels_path\n",
    "        \n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        Load data from a JSON file\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def get_query_dict(self, file):\n",
    "        \"\"\"\n",
    "        Count the number of occurrences of each query text in the data\n",
    "        \"\"\"\n",
    "        query_list = [data['query_text'] for data in file]\n",
    "        query_dict = defaultdict(int)\n",
    "        for query in query_list:\n",
    "            query_dict[query] += 1\n",
    "        return query_dict\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"\n",
    "        Preprocess the source sentences by adding the query text and replacing periods with commas\n",
    "        \"\"\"\n",
    "        data['source_snt'] = data['source_snt'].str.replace('.',',', regex=False)\n",
    "        data['source_snt'] = data[['source_snt', 'query_text']].agg(' related to '.join, axis=1)+\".\"\n",
    "        return data\n",
    "\n",
    "    def load_data_and_get_dataframe(self):\n",
    "        \"\"\"\n",
    "        Load the data and the query relevance judgments from JSON files and merge them into a single dataframe\n",
    "        \"\"\"\n",
    "        data = self.load_data(self.data_path)\n",
    "        query_dict = self.get_query_dict(data)\n",
    "        query_json = json.dumps(query_dict, indent=4)\n",
    "        qrels = self.load_data(self.qrels_path)\n",
    "        merged_data = pd.concat([pd.DataFrame(data), pd.DataFrame(qrels)], axis=1)\n",
    "        preprocessed_data = self.preprocess_data(merged_data)\n",
    "        df = preprocessed_data[['source_snt', 'simplified_snt']]\n",
    "        return data, query_dict, query_json, df\n",
    "\n",
    "    def get_max_and_avg_length(self, data, column_name):\n",
    "        \"\"\"\n",
    "        Compute the maximum and average length of the sentences in a given column of the dataframe\n",
    "        \"\"\"\n",
    "        lengths = [len(s.split()) for s in data[column_name]]\n",
    "        max_length = max(lengths)\n",
    "        average_length = np.mean(lengths)\n",
    "        return max_length, average_length\n",
    "\n",
    "    def print_data_info(self):\n",
    "        \"\"\"\n",
    "        Load the data, preprocess it, and compute various statistics on it\n",
    "        \"\"\"\n",
    "        data, query_dict, query_json, df = self.load_data_and_get_dataframe()\n",
    "        print(f\"Data size: {len(data)}\")\n",
    "        print(f\"The number of query texts: {len(query_dict)}\")\n",
    "        print(f\"The amount of data per query text: {query_json}\")\n",
    "    \n",
    "        max_length, average_length = self.get_max_and_avg_length(df, 'source_snt')\n",
    "        print('Max length of source sentence: ',max_length,'\\n'+'Avg length of source sentence: ',average_length)\n",
    "\n",
    "        max_length, average_length = self.get_max_and_avg_length(df, 'simplified_snt')\n",
    "        print('Max length of simplified sentence: ',max_length,'\\n'+'Avg length of simplified sentence ',average_length)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07bbe531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 648\n",
      "The number of query texts: 43\n",
      "The amount of data per query text: {\n",
      "    \"drones\": 31,\n",
      "    \"self driving\": 132,\n",
      "    \"misinformation\": 63,\n",
      "    \"cryptocurrency\": 41,\n",
      "    \"forensics\": 54,\n",
      "    \"Digital assistant\": 2,\n",
      "    \"Biases\": 6,\n",
      "    \"humanoid robots\": 34,\n",
      "    \"digital marketing\": 13,\n",
      "    \"Privacy\": 7,\n",
      "    \"smart speaker\": 17,\n",
      "    \"drug discovery\": 8,\n",
      "    \"functional genomics\": 3,\n",
      "    \"infectious diseases\": 5,\n",
      "    \"gene editing\": 20,\n",
      "    \"crispr\": 4,\n",
      "    \"conspiracy theories\": 18,\n",
      "    \"financial markets\": 11,\n",
      "    \"Alcohol interfer with recovery and adaptation to training\": 32,\n",
      "    \"The problem Of muscle hypertrophy: Revisited\": 5,\n",
      "    \"Occlusion Training for Hypertrophy\": 5,\n",
      "    \"Hypertrophy after Aerobic Exercise\": 3,\n",
      "    \"Stretch training induces hypertrophy?\": 7,\n",
      "    \"Cycle training induces muscle hypertrophy\": 7,\n",
      "    \"Volume for hypertrophy\": 7,\n",
      "    \"short versus long inter-set rest intervals on hypertrophy\": 4,\n",
      "    \"Muscle memory\": 9,\n",
      "    \"Effects of meal frequency on weight loss\": 6,\n",
      "    \"Resistance Training is Medicine\": 7,\n",
      "    \"Benefits of strength training for adolescents\": 7,\n",
      "    \"Resistance training for muscle function deficits\": 6,\n",
      "    \"Effect of Repetition Duration on Hypertrophy\": 7,\n",
      "    \"The cardiovascular system after exercise\": 9,\n",
      "    \"\\\"Exercise as medicine\\\" in chronic kidney disease\": 5,\n",
      "    \"blood flow restriction in individuals with knee pain\": 6,\n",
      "    \"Prescription of exercise training for hypertensives\": 6,\n",
      "    \"Effects of intermittent fasting on sport performance\": 3,\n",
      "    \"Maintaining Physical Performance\": 6,\n",
      "    \"post-exercise anabolic window\": 8,\n",
      "    \"No Time to Lift? Designing Time\\u2011Efficient Training Programs\": 8,\n",
      "    \"How many training per week for hypetrophy?\": 6,\n",
      "    \"Training Techniques for hypertrophy\": 3,\n",
      "    \"Mechanisms of Muscle Hypertrophy\": 7\n",
      "}\n",
      "Max length of source sentence:  71 \n",
      "Avg length of source sentence:  28.734567901234566\n",
      "Max length of simplified sentence:  61 \n",
      "Avg length of simplified sentence  18.814814814814813\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_snt</th>\n",
       "      <th>simplified_snt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the modern era of automation and robotics, ...</td>\n",
       "      <td>Current academic and industrial research is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the ever increasing number of unmanned ae...</td>\n",
       "      <td>Drones are increasingly used in the civilian a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Due to guidelines set by the governments regar...</td>\n",
       "      <td>Governments set guidelines on the operation ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In an attempt to achieve the above mentioned t...</td>\n",
       "      <td>Researchers propose data-driven solutions allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Derived from the classic image classification ...</td>\n",
       "      <td>The algorithm, based on the Inception model, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>Bodybuilders generally train with moderate loa...</td>\n",
       "      <td>Bodybuilders train with moderate loads and sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Powerlifters, on the other hand, routinely tra...</td>\n",
       "      <td>Powerlifters, on the other hand, train with hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>Although both groups are known to display impr...</td>\n",
       "      <td>Although both groups are known to display impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>It has been shown that many factors mediate th...</td>\n",
       "      <td>It has been shown that many factors mediate th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>Therefore, the purpose of this paper is twofol...</td>\n",
       "      <td>Therefore, the purpose of this paper is to rev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            source_snt  \\\n",
       "0    In the modern era of automation and robotics, ...   \n",
       "1    With the ever increasing number of unmanned ae...   \n",
       "2    Due to guidelines set by the governments regar...   \n",
       "3    In an attempt to achieve the above mentioned t...   \n",
       "4    Derived from the classic image classification ...   \n",
       "..                                                 ...   \n",
       "643  Bodybuilders generally train with moderate loa...   \n",
       "644  Powerlifters, on the other hand, routinely tra...   \n",
       "645  Although both groups are known to display impr...   \n",
       "646  It has been shown that many factors mediate th...   \n",
       "647  Therefore, the purpose of this paper is twofol...   \n",
       "\n",
       "                                        simplified_snt  \n",
       "0    Current academic and industrial research is in...  \n",
       "1    Drones are increasingly used in the civilian a...  \n",
       "2    Governments set guidelines on the operation ce...  \n",
       "3    Researchers propose data-driven solutions allo...  \n",
       "4    The algorithm, based on the Inception model, d...  \n",
       "..                                                 ...  \n",
       "643  Bodybuilders train with moderate loads and sho...  \n",
       "644  Powerlifters, on the other hand, train with hi...  \n",
       "645  Although both groups are known to display impr...  \n",
       "646  It has been shown that many factors mediate th...  \n",
       "647  Therefore, the purpose of this paper is to rev...  \n",
       "\n",
       "[648 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = DataProcessor()\n",
    "df = dp.print_data_info()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789453b0",
   "metadata": {},
   "source": [
    " ## Text Complexity Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e98cdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Crystal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Crystal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli import download, link\n",
    "from spacy.util import is_package, get_package_path\n",
    "from source.helper import yield_lines, load_dump, dump, tokenize, generate_hash\n",
    "from source.resources import download_glove, DUMPS_DIR\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b04b8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextComplexityScorer:\n",
    "    def __init__(self):\n",
    "        self.MODEL = 'en_core_web_sm'\n",
    "        self.DUMPS_DIR = DUMPS_DIR\n",
    "        self.WORD_EMBEDDINGS_NAME = \"glove.42B.300d\"\n",
    "        self.word2rank = None\n",
    "        \n",
    "    @lru_cache(maxsize=1)\n",
    "    def get_spacy_model():\n",
    "        model = self.MODEL\n",
    "        if not spacy.util.is_package(model):\n",
    "            spacy.cli.download(model)\n",
    "            spacy.cli.link(model, model, force=True, model_path=spacy.util.get_package_path(model))\n",
    "        return spacy.load(model)\n",
    "    \n",
    "    @lru_cache(maxsize=10 ** 6)\n",
    "    def spacy_process(self, text):\n",
    "        \"\"\"\n",
    "        Pre-processes the text using the spacy library.\n",
    "        \"\"\"\n",
    "        return get_spacy_model()(text)\n",
    "\n",
    "    @lru_cache(maxsize=1024)\n",
    "    def get_dependency_tree_depth(self, sentence):\n",
    "        \"\"\"\n",
    "        Computes the dependency tree depth of the given sentence.\n",
    "        \"\"\"\n",
    "        def tree_height(node):\n",
    "            if len(list(node.children)) == 0:\n",
    "                return 0\n",
    "            return 1 + max(tree_height(child) for child in node.children)\n",
    "\n",
    "        tree_depths = [tree_height(spacy_sentence.root) for spacy_sentence in self.spacy_process(sentence).sents]\n",
    "        if len(tree_depths) == 0:\n",
    "            return 0\n",
    "        return max(tree_depths)\n",
    "\n",
    "    @lru_cache(maxsize=1)\n",
    "    def get_word2rank(self, vocab_size=np.inf):\n",
    "        \"\"\"\n",
    "        Downloads and pre-processes the GloVe word embeddings.\n",
    "        \"\"\"\n",
    "        model_filepath = self.DUMPS_DIR / f\"{self.WORD_EMBEDDINGS_NAME}.pk\"\n",
    "        if model_filepath.exists():\n",
    "            return load_dump(model_filepath)\n",
    "\n",
    "        print(\"Downloading glove.42B.300d ...\")\n",
    "        download_glove(model_name=self.WORD_EMBEDDINGS_NAME, dest_dir=str(self.DUMPS_DIR))\n",
    "        print(\"Preprocessing word2rank...\")\n",
    "\n",
    "        word2rank = {}\n",
    "        with yield_lines(self.DUMPS_DIR / f\"{self.WORD_EMBEDDINGS_NAME}.txt\") as lines_generator:\n",
    "            for i, line in enumerate(lines_generator):\n",
    "                if vocab_size is not None and i >= vocab_size:\n",
    "                    break\n",
    "                word = line.split(' ')[0]\n",
    "                word2rank[word] = i\n",
    "                \n",
    "        dump(word2rank, model_filepath)\n",
    "        txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "        zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "        if txt_file.exists(): txt_file.unlink()\n",
    "        if zip_file.exists(): zip_file.unlink()\n",
    "        return word2rank\n",
    "\n",
    "    @lru_cache(maxsize=5000)\n",
    "    def get_rank(self, word, normalized=False):\n",
    "        \"\"\"\n",
    "        Computes the rank of the given word in the word2rank vocabulary.\n",
    "        \"\"\"\n",
    "        if self.word2rank is None:\n",
    "            self.word2rank = self.get_word2rank()\n",
    "        max_rank = len(self.word2rank)\n",
    "\n",
    "        rank = self.word2rank.get(word, max_rank)\n",
    "        rank = np.log(1 + rank)\n",
    "        if normalized:\n",
    "            rank = rank / np.log(1 + max_rank)\n",
    "\n",
    "        return rank\n",
    "\n",
    "    @lru_cache(maxsize=2048)\n",
    "    def get_word_complexity_score(self, sentence):\n",
    "        \"\"\"\n",
    "        Computes the complexity score of the given sentence based on the ranks of its words in the word2rank vocabulary.\n",
    "        \"\"\"\n",
    "        words = [word.lower() for word in tokenize(sentence)\n",
    "                 if word.lower() not in stopwords and\n",
    "                 not all(char in string.punctuation for char in word)]\n",
    "\n",
    "        words = [word for word in words if word in self.get_word2rank()]\n",
    "\n",
    "        if not words:\n",
    "            return 1.0\n",
    "        return np.quantile([self.get_rank(word) for word in words], 0.75)\n",
    "\n",
    "    def get_text_complexity(self, simple_sentences, complex_sentences):\n",
    "        \"\"\"\n",
    "        Computes the complexity score of the given text\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for i in range(len(simple_sentences)):\n",
    "            simple_score = self.get_word_complexity_score(simple_sentences[i])\n",
    "            complex_score = self.get_word_complexity_score(complex_sentences[i])\n",
    "            if complex_score == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = simple_score / complex_score\n",
    "            score = min(float(f'%.2f' % score), 1.0)\n",
    "            scores.append(score)\n",
    "        return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9274252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.829503794772458"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexity_scorer = TextComplexityScorer()\n",
    "rank = complexity_scorer.get_word_complexity_score('Current academic and industrial research is interested in autonomous vehicles.')\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eeec48ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sentences=['Current academic and industrial research is interested in autonomous vehicles.']\n",
    "complex_sentences=['In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.']\n",
    "complexity_scorer.get_text_complexity(simple_sentences,complex_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de9149",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "068a67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatioFeature:\n",
    "    def __init__(self, feature_extractor, target_ratio=0.80):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.target_ratio = str(target_ratio)\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        return f'{self.name}_{self.target_ratio}'\n",
    "\n",
    "    def encode_sentence_pair(self, complex_sentence, simple_sentence):\n",
    "        return f'{self.name}_{self.feature_extractor(complex_sentence, simple_sentence)}', simple_sentence\n",
    "\n",
    "    def decode_sentence(self, encoded_sentence):\n",
    "        return encoded_sentence\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        class_name = self.__class__.__name__.replace('RatioFeature', '')\n",
    "        name = \"\"\n",
    "        for word in re.findall('[A-Z][^A-Z]*', class_name):\n",
    "            if word: name += word[0]\n",
    "        if not name: name = class_name\n",
    "        return name\n",
    "    \n",
    "    \n",
    "class WordRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_word_length_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_word_length_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(len(tokenize(simple_sentence)), len(tokenize(complex_sentence)))\n",
    "\n",
    "    \n",
    "class CharRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_char_length_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_char_length_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(len(simple_sentence), len(complex_sentence))\n",
    "\n",
    "\n",
    "class LevenshteinRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_levenshtein_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_levenshtein_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(Levenshtein.ratio(complex_sentence, simple_sentence))\n",
    "\n",
    "\n",
    "class WordRankRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_word_rank_ratio, *args, **kwargs)\n",
    "        self.complexity_scorer = complexity_scorer\n",
    "\n",
    "    def get_word_rank_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(min(self.complexity_scorer.get_word_complexity_score(simple_sentence),\n",
    "                                       self.complexity_scorer.get_word_complexity_score(complex_sentence)), 2)\n",
    "    \n",
    "    @lru_cache(maxsize=2048)\n",
    "    def get_lexical_complexity_score(self, sentence):\n",
    "        words = tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "        words = [word for word in words if word in get_word2rank()]\n",
    "        if len(words) == 0:\n",
    "            return np.log(1 + len(get_word2rank()))\n",
    "        return np.quantile([self.get_rank(word) for word in words], 0.75)\n",
    "\n",
    "    @lru_cache(maxsize=5000)\n",
    "    def get_rank(self, word):\n",
    "        rank = get_word2rank().get(word, len(get_word2rank()))\n",
    "        return np.log(1 + rank)\n",
    "\n",
    "\n",
    "class DependencyTreeDepthRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.complexity_scorer = complexity_scorer\n",
    "        super().__init__(self.get_dependency_tree_depth_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_dependency_tree_depth_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(\n",
    "                            self.get_dependency_tree_depth(simple_sentence),\n",
    "                          self.get_dependency_tree_depth(complex_sentence))\n",
    "    \n",
    "    @lru_cache(maxsize=1024)\n",
    "    def get_dependency_tree_depth(self, sentence):\n",
    "        def get_subtree_depth(node):\n",
    "            if len(list(node.children)) == 0:\n",
    "                return 0\n",
    "            return 1 + max([get_subtree_depth(child) for child in node.children])\n",
    "\n",
    "        tree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in self.spacy_process(sentence).sents]\n",
    "        if len(tree_depths) == 0:\n",
    "            return 0\n",
    "        return max(tree_depths)\n",
    "    \n",
    "    @lru_cache(maxsize=10 ** 6)\n",
    "    def spacy_process(self, text):\n",
    "        return complexity_scorer.spacy_process(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f85eac",
   "metadata": {},
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03f304f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, features_kwargs=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = self.get_features(features_kwargs)\n",
    "        if features_kwargs:\n",
    "            self.hash = generate_hash(str(features_kwargs).encode())\n",
    "        else:\n",
    "            self.hash = \"no_feature\"\n",
    "\n",
    "    def get_class(self, class_name, *args, **kwargs):\n",
    "        return globals()[class_name](*args, **kwargs)\n",
    "\n",
    "    def get_features(self, feature_kwargs):\n",
    "        features = []\n",
    "        for feature_name, kwargs in feature_kwargs.items():\n",
    "            features.append(self.get_class(feature_name, **kwargs))\n",
    "        return features\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        if self.features:\n",
    "            line = ''\n",
    "            for feature in self.features:\n",
    "                line += feature.encode_sentence(sentence) + ' '\n",
    "            line += sentence\n",
    "            return line.rstrip()\n",
    "        else:\n",
    "            return sentence\n",
    "\n",
    "    def encode_sentence_pair(self, complex_sentence, simple_sentence):\n",
    "        if self.features:\n",
    "            line = ''\n",
    "            for feature in self.features:\n",
    "                processed_complex, _ = feature.encode_sentence_pair(complex_sentence, simple_sentence)\n",
    "                line += processed_complex + ' '\n",
    "            line += complex_sentence\n",
    "            return line.rstrip()\n",
    "\n",
    "        else:\n",
    "            return complex_sentence\n",
    "\n",
    "    def decode_sentence(self, encoded_sentence):\n",
    "        for feature in self.features:\n",
    "            decoded_sentence = feature.decode_sentence(encoded_sentence)\n",
    "        return decoded_sentence\n",
    "\n",
    "    def encode_file(self, input_filepath, output_filepath):\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            for line in yield_lines(input_filepath):\n",
    "                f.write(self.encode_sentence(line) + '\\n')\n",
    "\n",
    "    def decode_file(self, input_filepath, output_filepath):\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            for line in yield_lines(input_filepath):\n",
    "                f.write(self.decode_sentence(line) + '\\n')\n",
    "\n",
    "    def encode_dataframe(self,dataset):\n",
    "        processed_complex_sentences = []\n",
    "        for complex_sentence, simple_sentence in tqdm(zip(dataset['source_snt'], dataset['simplified_snt']),total=len(dataset)):\n",
    "            processed_complex_sentence = self.encode_sentence_pair(complex_sentence, simple_sentence)\n",
    "            processed_complex_sentences.append(processed_complex_sentence)\n",
    "        return processed_complex_sentences\n",
    "\n",
    "    def preprocess_dataset(self, dataset):\n",
    "        new_df=dataset.copy()\n",
    "        new_df['source_snt']= self.encode_dataframe(dataset)\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4b2e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_kwargs = {\n",
    "        'WordRatioFeature': {'target_ratio': '1.05'},\n",
    "        'CharRatioFeature': {'target_ratio': '0.95'},\n",
    "        'LevenshteinRatioFeature': {'target_ratio': '0.75'},\n",
    "        'WordRankRatioFeature': {'target_ratio': '0.85'},\n",
    "        'DependencyTreeDepthRatioFeature': {'target_ratio': '0.85'}\n",
    "    }\n",
    "# features_kwargs = {}\n",
    "preprocessor = Preprocessor(features_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9743cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W_1.05 C_0.95 L_0.75 WR_0.85 DTD_0.85 In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(features_kwargs)\n",
    "# preprocessor.encode_sentence_pair('In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.','Current academic and industrial research is interested in autonomous vehicles.')\n",
    "preprocessor.encode_sentence('In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11a6c04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sentence='Current academic and industrial research is interested in autonomous vehicles.'\n",
    "complex_sentence='In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.'\n",
    "complexity_scorer.get_text_complexity(simple_sentences,complex_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98f1870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 4/648 [00:13<36:59,  3.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-95d01d0c19e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-0438d3371c7d>\u001b[0m in \u001b[0;36mpreprocess_dataset\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mnew_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mnew_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source_snt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-0438d3371c7d>\u001b[0m in \u001b[0;36mencode_dataframe\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mprocessed_complex_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcomplex_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimple_sentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source_snt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'simplified_snt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mprocessed_complex_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_sentence_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplex_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimple_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[0mprocessed_complex_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_complex_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprocessed_complex_sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-0438d3371c7d>\u001b[0m in \u001b[0;36mencode_sentence_pair\u001b[1;34m(self, complex_sentence, simple_sentence)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mprocessed_complex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_sentence_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplex_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimple_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0mline\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprocessed_complex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcomplex_sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-7cb8cd08c6c0>\u001b[0m in \u001b[0;36mencode_sentence_pair\u001b[1;34m(self, complex_sentence, simple_sentence)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode_sentence_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplex_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimple_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;34mf'{self.name}_{self.feature_extractor(complex_sentence, simple_sentence)}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimple_sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-7cb8cd08c6c0>\u001b[0m in \u001b[0;36mget_dependency_tree_depth_ratio\u001b[1;34m(self, complex_sentence, simple_sentence)\u001b[0m\n\u001b[0;32m     78\u001b[0m         return round(\n\u001b[0;32m     79\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dependency_tree_depth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimple_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                           self.get_dependency_tree_depth(complex_sentence))\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-7cb8cd08c6c0>\u001b[0m in \u001b[0;36mget_dependency_tree_depth\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_subtree_depth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mtree_depths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_subtree_depth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspacy_sentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mspacy_sentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspacy_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree_depths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-7cb8cd08c6c0>\u001b[0m in \u001b[0;36mspacy_process\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mspacy_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexity_scorer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspacy_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-de62babd929b>\u001b[0m in \u001b[0;36mspacy_process\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     23\u001b[0m             spacy.cli.link(self.MODEL, self.MODEL, force=True,\n\u001b[0;32m     24\u001b[0m                             model_path=spacy.util.get_package_path(self.MODEL))\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[1;32m---> 54\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"blank:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# installed as package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    466\u001b[0m     \"\"\"\n\u001b[0;32m    467\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\en_core_web_sm\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m     return load_model_from_path(\n\u001b[0m\u001b[0;32m    650\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[0mmeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     )\n\u001b[1;32m--> 514\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[0;32m   2138\u001b[0m             \u001b[1;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2139\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2140\u001b[1;33m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m  \u001b[1;31m# type: ignore[assignment]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2142\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_link_components\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1350\u001b[0m         \u001b[1;31m# Split to support file names like meta.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1352\u001b[1;33m             \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1353\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m   2124\u001b[0m         \u001b[0mdeserializers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"meta.json\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeserialize_meta\u001b[0m  \u001b[1;31m# type: ignore[assignment]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2125\u001b[0m         \u001b[0mdeserializers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeserialize_vocab\u001b[0m  \u001b[1;31m# type: ignore[assignment]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2126\u001b[1;33m         deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n\u001b[0m\u001b[0;32m   2127\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2128\u001b[0m         )\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.from_disk\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.from_bytes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.faster_heuristics.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._reload_special_cases\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._load_special_cases\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\spacy\\tokens\\_dict_proxies.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, doc, items)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Doc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpanGroup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     ) -> None:\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mUserDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_df = preprocessor.preprocess_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b16d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438dadc",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81642038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_l=load_data('test/simpletext-task3-test-large.json')\n",
    "# test_m=load_data('test/simpletext-task3-test-medium.json')\n",
    "test_s=load_data('test/simpletext-task3-test-small.json')\n",
    "def load_test_set(file):\n",
    "    dataset=pd.DataFrame(file,dtype='object')\n",
    "    for index, row in dataset.iterrows():\n",
    "        if not pd.isnull(row['query_text']):\n",
    "            dataset.at[index, 'source_snt'] = row['source_snt'].replace('.', ',') + ' related to ' + row['query_text'] + '.'\n",
    "    dataset=dataset[['source_snt']]\n",
    "    return dataset\n",
    "test_small=load_test_set(test_s)\n",
    "# test_medium=load_test_set(test_m)\n",
    "# test_large=load_test_set(test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a rich console logger\n",
    "console = Console(record=True)\n",
    "\n",
    "# to display dataframe in ASCII format\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        Column(\"source_text\", justify=\"left\"),\n",
    "        Column(\"target_text\", justify=\"left\"),\n",
    "        title=\"Sample Data\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5ef18",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "        row_source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = \" \".join(row_source_text.split())\n",
    "        target_text = \" \".join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [target_text],\n",
    "            max_length=self.summ_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_ids = target[\"input_ids\"].squeeze()\n",
    "        target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_text\":row_source_text,\n",
    "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
    "            \"target_mask\": target_mask.to(dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, source_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            source_text (str): column name of source text\n",
    "\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.source_text = self.data[source_text]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.source_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks\"\"\"\n",
    "\n",
    "        source_text = str(self.source_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = \" \".join(source_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c9fd8",
   "metadata": {},
   "source": [
    "## Train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfc72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training logger to log training progress\n",
    "def training_logger_init():\n",
    "    training_logger = Table(\n",
    "        Column(\"Epoch\", justify=\"center\"),\n",
    "        Column(\"Steps\", justify=\"center\"),\n",
    "        Column(\"Loss\", justify=\"center\"),\n",
    "        Column(\"ROUGE_Loss\", justify=\"center\"),\n",
    "        Column(\"ROUGE_1\", justify=\"center\"),\n",
    "        Column(\"ROUGE_2\", justify=\"center\"),\n",
    "        Column(\"ROUGE_L\", justify=\"center\"),\n",
    "        Column(\"SARI\", justify=\"center\"),\n",
    "        Column(\"BLEU\", justify=\"center\"),\n",
    "        Column(\"FKGL\", justify=\"center\"),\n",
    "        title=\"Training Status\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "    return training_logger\n",
    "\n",
    "# training logger to log training progress\n",
    "def epoch_training_logger_init():\n",
    "    epoch_training_logger = Table(\n",
    "        Column(\"Epoch\", justify=\"center\"),\n",
    "        Column(\"Train_Loss\", justify=\"center\"),\n",
    "        Column(\"Valid_Loss\", justify=\"center\"),\n",
    "        Column(\"ROUGE_1\", justify=\"center\"),\n",
    "        Column(\"ROUGE_2\", justify=\"center\"),\n",
    "        Column(\"ROUGE_L\", justify=\"center\"),\n",
    "        Column(\"SARI\", justify=\"center\"),\n",
    "        Column(\"BLEU\", justify=\"center\"),\n",
    "        Column(\"FKGL\", justify=\"center\"),\n",
    "        title=\"Training Epoch Status\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "    return epoch_training_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0577a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "from evaluate import load\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_rouge(predicted, target):\n",
    "    rouger = Rouge()\n",
    "    scores = rouger.get_scores(predicted, target, avg=True)\n",
    "    return scores['rouge-1']['f'],scores['rouge-2']['f'],scores['rouge-l']['f']\n",
    "\n",
    "def compute_bleu(predicted, target):\n",
    "    bleu_scores = []\n",
    "    for i in range(len(predicted)):\n",
    "        bleu_scores.append(sentence_bleu([target[i]],predicted[i], weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "def compute_sari(sources, predicted, target):\n",
    "    sari = load(\"sari\")\n",
    "    sari_scores=sari.compute(sources=sources, predictions=predicted, references=[[i] for i in target])\n",
    "    return sari_scores['sari']\n",
    "\n",
    "sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "predictions=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "references=[\"About 95 species are currently known.\",\"About 95 species are currently known.\"]\n",
    "compute_bleu(predictions,references)\n",
    "# compute_sari(sources,predictions,references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.sari import corpus_sari\n",
    "from evaluation.bleu import corpus_bleu\n",
    "from evaluation.fkgl import corpus_fkgl\n",
    "sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "predictions=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "references=[\"About 95 species are currently known.\",\"About 95 species are currently known.\"]\n",
    "corpus_sari(sources,predictions,[references],lowercase=True)\n",
    "corpus_bleu(predictions,[references],lowercase=True)\n",
    "# corpus_fkgl(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer,model_params):\n",
    "    model.train()\n",
    "    total_loss=[]\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "        target_mask = data[\"target_mask\"].to(device, dtype=torch.long)[:, 1:]\n",
    "        source_text = data[\"source_text\"]\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "            decoder_attention_mask=target_mask,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "                  input_ids = ids,\n",
    "                  attention_mask = mask, \n",
    "                  max_length=model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "                  num_beams=model_params[\"NUM_BEAMS\"],\n",
    "                  repetition_penalty=model_params[\"REPETITION_PENALTY\"], \n",
    "                  length_penalty=model_params[\"LENGTH_PENALTY\"], \n",
    "                  early_stopping=True,\n",
    "                  do_sample=False,\n",
    "                  temperature=0.25,\n",
    "                  top_k=120,\n",
    "                  top_p=0.98,\n",
    "                  )\n",
    "\n",
    "        # calculate rouge score\n",
    "        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "        target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "        rouge1,rouge2,rougeL = calculate_rouge(preds, target)\n",
    "#         bleu_score = compute_bleu(preds, target)\n",
    "#         sari_score = compute_sari(source_text,preds, target)\n",
    "        sari_score=corpus_sari(source_text,preds, [target],lowercase=False)\n",
    "        bleu_score=corpus_bleu(preds, [target],lowercase=False)\n",
    "        fkgl_score=corpus_fkgl(preds)\n",
    "\n",
    "        # add rouge loss to total loss\n",
    "        rouge_loss = 1.0 - np.mean([rouge1,rouge2,rougeL])\n",
    "        complexity_score = get_lexical_complexity_score_batch(target,preds)#higher is better\n",
    "        \n",
    "        lambda_ = 0.7\n",
    "        loss = lambda_ * loss + (1-lambda_)*(1-complexity_score)\n",
    "\n",
    "        if _ % 10 == 0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss),str(rouge_loss),str(rouge1),str(rouge2),str(rougeL),str(sari_score),str(bleu_score),str(fkgl_score))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         (loss + rouge_loss).backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss)\n",
    "    average_loss=sum(total_loss)/len(total_loss)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(tokenizer, model, device, loader,model_params):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    total_loss=[]\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    bleu_scores = []\n",
    "    sari_scores = []\n",
    "    fkgl_scores = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "            y_ids = y[:, :-1].contiguous()\n",
    "            lm_labels = y[:, 1:].clone().detach()\n",
    "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "            target_mask = data[\"target_mask\"].to(device, dtype=torch.long)[:, 1:]\n",
    "            source_text = data[\"source_text\"]\n",
    "            \n",
    "            outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "            decoder_attention_mask=target_mask,\n",
    "            )\n",
    "            loss = outputs[0]\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                  input_ids = ids,\n",
    "                  attention_mask = mask, \n",
    "                  max_length=model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "                  num_beams=model_params[\"NUM_BEAMS\"],\n",
    "                  repetition_penalty=model_params[\"REPETITION_PENALTY\"], \n",
    "                  length_penalty=model_params[\"LENGTH_PENALTY\"], \n",
    "                  early_stopping=True,\n",
    "                  do_sample=False,\n",
    "                  temperature=0.25,\n",
    "                  top_k=120,\n",
    "                  top_p=0.98,\n",
    "                  )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            rouge1,rouge2,rougeL = calculate_rouge(preds, target)\n",
    "            sari_score=corpus_sari(source_text,preds, [target],lowercase=False)\n",
    "            bleu_score=corpus_bleu(preds, [target],lowercase=False)\n",
    "            fkgl_score=corpus_fkgl(preds)\n",
    "#             bleu_score = compute_bleu(preds, target)\n",
    "#             sari_score = compute_sari(source_text,preds, target)\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            \n",
    "            total_loss.append(loss)\n",
    "            rouge1_scores.append(rouge1)\n",
    "            rouge2_scores.append(rouge2)\n",
    "            rougeL_scores.append(rougeL)\n",
    "            sari_scores.append(sari_score)\n",
    "            bleu_scores.append(bleu_score)\n",
    "            fkgl_scores.append(fkgl_score)\n",
    "            \n",
    "    average_loss=sum(total_loss)/len(total_loss)\n",
    "    average_rouge1=sum(rouge1_scores)/len(rouge1_scores)\n",
    "    average_rouge2=sum(rouge2_scores)/len(rouge2_scores)\n",
    "    average_rougeL=sum(rougeL_scores)/len(rougeL_scores)\n",
    "    average_bleu=sum(bleu_scores)/len(bleu_scores)\n",
    "    average_sari=sum(sari_scores)/len(sari_scores)\n",
    "    average_fkgl=sum(fkgl_scores)/len(fkgl_scores)\n",
    "    return predictions, actuals,average_loss, average_rouge1,average_rouge2,average_rougeL,average_bleu,average_sari,average_fkgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e885397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(tokenizer, model, device, loader,model_params):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to test model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "                num_beams=model_params[\"NUM_BEAMS\"],\n",
    "                repetition_penalty=model_params[\"REPETITION_PENALTY\"], \n",
    "                length_penalty=model_params[\"LENGTH_PENALTY\"], \n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "                temperature=0.25,\n",
    "                top_k=120,\n",
    "                top_p=0.98,\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            if _%10==0:\n",
    "                console.print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea4f1b",
   "metadata": {},
   "source": [
    "## ModelTrainer and ModelTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelTrainer(\n",
    "    dataframe, source_text, target_text, model,tokenizer,model_params, output_dir\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "    model = model.to(device)\n",
    "\n",
    "     # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text, target_text]]\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "    train_size = 0.8\n",
    "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "    train_dataset[\"source_snt\"] = \"simplify: \" + train_dataset[\"source_snt\"]\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    \n",
    "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    if model_params[\"CONTROL_TOKENS\"]:\n",
    "        val_dataset[\"source_snt\"]=[preprocessor.encode_sentence(i[38:]) for i in val_dataset[\"source_snt\"]]\n",
    "    val_dataset[\"source_snt\"] = \"simplify: \" + val_dataset[\"source_snt\"]\n",
    "    \n",
    "    display_df(train_dataset.head(2))\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"VALID Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = TrainDataSetClass(\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    val_set = TrainDataSetClass(\n",
    "        val_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        train_loss =train(epoch, tokenizer, model, device, training_loader, optimizer,model_params)\n",
    "        predictions, actuals, valid_loss, average_rouge1,average_rouge2,average_rougeL,average_bleu,average_sari,average_fkgl = validate(tokenizer, model, device, val_loader,model_params)\n",
    "        epoch_training_logger.add_row(str(epoch), str(train_loss), str(valid_loss), str(average_rouge1),str(average_rouge2),str(average_rougeL),str(average_sari),str(average_bleu),str(average_fkgl))\n",
    "        console.print(epoch_training_logger)\n",
    "        \n",
    "    final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "    final_df.to_csv(os.path.join(output_dir, \"valid_predictions.csv\"))\n",
    "\n",
    "    console.log(f\"[Saving Model]...\\n\")\n",
    "    # Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
    "\n",
    "    console.print(\n",
    "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
    "    )\n",
    "    console.print(\n",
    "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'valid_predictions.csv')}\\n\"\"\"\n",
    "    )\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelTest(\n",
    "   test,size,source_text,model,tokenizer,model_params, output_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    T5 test\n",
    "\n",
    "    \"\"\"        \n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Testing {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading {size} test data...\\n\")\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    test = test.reset_index(drop=True)\n",
    "    console.print(f\"Test {size} Dataset: {test.shape}\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    test_set = TestDataSetClass(\n",
    "        test,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    test_params = {\n",
    "        \"batch_size\": model_params[\"TEST_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "    # Testing loop\n",
    "    console.log(f\"[Initiating Testing]...\\n\")\n",
    "    predictions = testing(tokenizer, model, device, test_loader,model_params)\n",
    "    final_df = pd.DataFrame({\"simplified_snt\": predictions})\n",
    "    final_df.to_csv(os.path.join(output_dir, f\"test_{size}_predictions.csv\"))\n",
    "\n",
    "    console.log(f\"[Testing Completed.]\\n\")\n",
    "    console.print(\n",
    "        f\"\"\"[Test] Generation on Test data saved @ {os.path.join(output_dir,f'test_{size}_predictions.csv')}\\n\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_set,model,tokenizer,size,output_dir):\n",
    "    test_set=test_set.copy()\n",
    "    if model_params[\"CONTROL_TOKENS\"]:\n",
    "        test_set[\"source_snt\"]=[preprocessor.encode_sentence(i) for i in test_set[\"source_snt\"]]\n",
    "    test_set[\"source_snt\"] = \"simplify: \" + test_set[\"source_snt\"]\n",
    "    ModelTest(\n",
    "        test=test_set,\n",
    "        source_text=\"source_snt\",\n",
    "        size=size,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        model_params=model_params,\n",
    "        output_dir=output_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a400a",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2048f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(output_dir,model_class,tokenizer_class,model_name):\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    model_config = model_class.config_class.from_pretrained(model_name)\n",
    "\n",
    "    model_path = os.path.join(output_dir, \"model_files\")\n",
    "\n",
    "    model = model_class.from_pretrained(model_path, config=model_config)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_summary(text,model,tokenizer):\n",
    "    if model_params[\"CONTROL_TOKENS\"]:\n",
    "        text=preprocessor.encode_sentence(text)\n",
    "    input_ids = tokenizer.encode(\"simplify: \"+text, return_tensors='pt', \n",
    "                                    max_length=tokenizer.model_max_length, \n",
    "                                    truncation=True)\n",
    "    summary_ids = model.generate(input_ids, \n",
    "                                 max_length=model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "                                 num_beams=model_params[\"NUM_BEAMS\"],\n",
    "                                 repetition_penalty=model_params[\"REPETITION_PENALTY\"], \n",
    "                                 length_penalty=model_params[\"LENGTH_PENALTY\"],\n",
    "                                 do_sample=False,\n",
    "                                 temperature=0.25,\n",
    "                                 top_k=120,\n",
    "                                 top_p=0.98,)\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c666b",
   "metadata": {},
   "source": [
    "## T5 model with control tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2e74b",
   "metadata": {},
   "source": [
    "### T5 Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78b2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenzier for encoding the text\n",
    "model_params = {\n",
    "    \"MODEL\": \"t5-base\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TEST_BATCH_SIZE\": 8,  # test batch size\n",
    "    \"TRAIN_EPOCHS\": 5,  # number of training epochs\n",
    "    \"LEARNING_RATE\": 3e-4,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 100,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 75,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "    \"NUM_BEAMS\":8,\n",
    "    \"REPETITION_PENALTY\":2.5, \n",
    "    \"LENGTH_PENALTY\":1, \n",
    "    \"CONTROL_TOKENS\":True,\n",
    "}\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_params[\"MODEL\"])\n",
    "training_logger = training_logger_init()\n",
    "epoch_training_logger=epoch_training_logger_init()\n",
    "\n",
    "# control tokens\n",
    "features_kwargs = {\n",
    "        'WordRatioFeature': {'target_ratio': '1.05'},\n",
    "        'CharRatioFeature': {'target_ratio': '0.95'},\n",
    "        'LevenshteinRatioFeature': {'target_ratio': '0.75'},\n",
    "        'WordRankRatioFeature': {'target_ratio': '0.95'},\n",
    "        'DependencyTreeDepthRatioFeature': {'target_ratio': '0.85'}\n",
    "    }\n",
    "# features_kwargs = {}\n",
    "preprocessor = Preprocessor(features_kwargs)\n",
    "\n",
    "ModelTrainer(\n",
    "    dataframe=new_df,\n",
    "    source_text=\"source_snt\",\n",
    "    target_text=\"simplified_snt\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_params=model_params,\n",
    "    output_dir=\"T5_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46655fc",
   "metadata": {},
   "source": [
    "### T5 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208414f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model,tokenizer=load_model(\"T5_outputs\",T5ForConditionalGeneration,T5Tokenizer,\"t5-small\")\n",
    "test(test_small,model,tokenizer,'small',\"T5_outputs\")\n",
    "# test(test_medium,model,tokenizer,'medium',\"T5_outputs\")\n",
    "# test(test_medium,model,tokenizer,'large',\"T5_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model,tokenizer=load_model(\"T5_outputs\",T5ForConditionalGeneration,T5TokenizerFast,\"t5-base\")\n",
    "text='This is moving us to a tipping point and to a crossroads: we must decide between a society in which the actions are determined in a top-down way and then implemented by coercion or manipulative technologies (such as personalized ads and nudging) or a society, in which decisions are taken in a free and participatory way and mutually coordinated.'\n",
    "generate_summary(text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_format(size, run):\n",
    "    #load test results\n",
    "    file=load_data(f'test/simpletext-task3-test-{size}.json')\n",
    "\n",
    "    df1=pd.read_csv(f\"T5_outputs/test_{size}_predictions.csv\")\n",
    "    df2=pd.DataFrame(file)\n",
    "    snt_id=df2[['snt_id']]\n",
    "    \n",
    "    df1 = df1.drop(df1.columns[0], axis=1)\n",
    "    df1.insert(0, 'run_id', f'QH_task_3_run{run}')\n",
    "    df1.insert(1, 'manual', 0)\n",
    "    df1.insert(2, 'snt_id', snt_id)\n",
    "\n",
    "    df1.to_json(f'{size}_pre_with_id.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 1\n",
    "for size in ['small']:\n",
    "    output_format(size, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc452e",
   "metadata": {},
   "source": [
    "## T5 model without control tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenzier for encoding the text\n",
    "model_params = {\n",
    "    \"MODEL\": \"t5-small\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TEST_BATCH_SIZE\": 8,  # test batch size\n",
    "    \"TRAIN_EPOCHS\": 3,  # number of training epochs\n",
    "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 70,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 55,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "    \"NUM_BEAMS\":8,\n",
    "    \"REPETITION_PENALTY\":2.5, \n",
    "    \"LENGTH_PENALTY\":1, \n",
    "    \"CONTROL_TOKENS\":False,\n",
    "}\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_params[\"MODEL\"])\n",
    "training_logger = training_logger_init()\n",
    "epoch_training_logger=epoch_training_logger_init()\n",
    "\n",
    "# features_kwargs = {}\n",
    "preprocessor = Preprocessor(features_kwargs)\n",
    "\n",
    "ModelTrainer(\n",
    "    dataframe=df,\n",
    "    source_text=\"source_snt\",\n",
    "    target_text=\"simplified_snt\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_params=model_params,\n",
    "    output_dir=\"T5_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1eeba",
   "metadata": {},
   "source": [
    "## BART model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dc5e7",
   "metadata": {},
   "source": [
    "### BART training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46905214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenzier for encoding the text\n",
    "model_params = {\n",
    "    \"MODEL\": 'facebook/bart-large-cnn',  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TEST_BATCH_SIZE\": 8,  # test batch size\n",
    "    \"TRAIN_EPOCHS\": 1,  # number of training epochs\n",
    "    \"LEARNING_RATE\": 1e-5,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 100,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 75,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "    \"NUM_BEAMS\":4,\n",
    "    \"REPETITION_PENALTY\":2.5, \n",
    "    \"LENGTH_PENALTY\":0.75, \n",
    "}\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# 加载BART模型和分词器\n",
    "model = BartForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "tokenizer = BartTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "model.model.decoder.generation_mode=False\n",
    "\n",
    "training_logger = training_logger_init()\n",
    "epoch_training_logger=epoch_training_logger_init()\n",
    "\n",
    "train_data=df.copy()\n",
    "train_data[\"source_snt\"] = tokenizer.bos_token + train_data[\"source_snt\"] + tokenizer.eos_token\n",
    "ModelTrainer(\n",
    "    dataframe=train_data,\n",
    "    source_text=\"source_snt\",\n",
    "    target_text=\"simplified_snt\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_params=model_params,\n",
    "    output_dir=\"Bart_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb02813",
   "metadata": {},
   "source": [
    "### Bart Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model,tokenizer=load_model(\"Bart_outputs\")\n",
    "model,tokenizer=load_model(\"Bart_outputs\",BartForConditionalGeneration,BartTokenizer,'facebook/bart-large-cnn')\n",
    "test(test_small,model,tokenizer,'small',\"Bart_outputs\")\n",
    "test(test_medium,model,tokenizer,'medium',\"Bart_outputs\")\n",
    "test(test_medium,model,tokenizer,'large',\"Bart_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,tokenizer=load_model(\"Bart_outputs\",BartForConditionalGeneration,BartTokenizer,'facebook/bart-large-cnn')\n",
    "text='<s>In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.</s>'\n",
    "generate_summary(text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d0cd0",
   "metadata": {},
   "source": [
    "## FlaxT5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204412c",
   "metadata": {},
   "source": [
    "### Flax T5 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenzier for encoding the text\n",
    "model_params = {\n",
    "    \"MODEL\": 'google/t5-v1-1.1.0',  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TEST_BATCH_SIZE\": 8,  # test batch size\n",
    "    \"TRAIN_EPOCHS\": 1,  # number of training epochs\n",
    "    \"LEARNING_RATE\": 3e-5,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 100,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 75,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "    \"NUM_BEAMS\":4,\n",
    "    \"REPETITION_PENALTY\":2.5, \n",
    "    \"LENGTH_PENALTY\":0.75, \n",
    "}\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 加载预训练的GPT-2模型和tokenizer\n",
    "from transformers import FlaxT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = FlaxT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "training_logger = training_logger_init()\n",
    "epoch_training_logger=epoch_training_logger_init()\n",
    "\n",
    "\n",
    "ModelTrainer(\n",
    "    dataframe=train_data,\n",
    "    source_text=\"source_snt\",\n",
    "    target_text=\"simplified_snt\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_params=model_params,\n",
    "    output_dir=\"GPT2_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e082d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
