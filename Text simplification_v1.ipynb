{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a390d671",
   "metadata": {},
   "source": [
    "# T5 model for text simplification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279090df",
   "metadata": {},
   "source": [
    "## import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23440e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pip in /usr/local/anaconda3/lib/python3.8/site-packages (23.1.1)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in /usr/local/anaconda3/lib/python3.8/site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (1.8)\n",
      "Requirement already satisfied: networkx in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (2.5.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /usr/local/anaconda3/lib/python3.8/site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from networkx->torch) (4.4.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/anaconda3/lib/python3.8/site-packages (from sympy->torch) (1.2.1)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: SentencePiece in /usr/local/anaconda3/lib/python3.8/site-packages (0.1.98)\n",
      "Found existing installation: transformers 4.28.1\n",
      "Uninstalling transformers-4.28.1:\n",
      "  Successfully uninstalled transformers-4.28.1\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting transformers\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/d8/a7/a6ff727fd5d96d6625f4658944a2ae230f0c75743a9a117fbda013b03d3d/transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/anaconda3/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.28.1\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: rich in /usr/local/anaconda3/lib/python3.8/site-packages (13.3.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from rich) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from rich) (2.15.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from rich) (4.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich) (0.1.2)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: rouge in /usr/local/anaconda3/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.8/site-packages (from rouge) (1.16.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: evaluate in /usr/local/anaconda3/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (1.22.3)\n",
      "Requirement already satisfied: dill in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (1.2.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (2023.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (0.13.4)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (20.9)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/anaconda3/lib/python3.8/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/anaconda3/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from packaging->evaluate) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from pandas->evaluate) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from pandas->evaluate) (2021.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade torch\n",
    "!pip install SentencePiece\n",
    "!pip uninstall transformers -y\n",
    "!pip install transformers\n",
    "!pip install rich\n",
    "!pip install rouge\n",
    "!pip install evaluate\n",
    "!pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc01186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "# rich: for a better display on terminal\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55cc1f1",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3307c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sou_path):\n",
    "    with open(sou_path, 'r', encoding='utf-8') as sou_file:\n",
    "                # 循环读取源文件数据，并写入到目标文件中\n",
    "        file = json.load(sou_file)\n",
    "        return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affc1239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path: D:\\Program Files (x86)\\Anaconda3\\NLP project\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "print(\"Current path:\", current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6919b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=load_data('train/simpletext_task3_train.json')\n",
    "file2=load_data('train/simpletext_task3_qrels.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150b4308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 648\n",
      "The number of query text: 43\n",
      "The amount of data per query text: {'drones': 31, 'self driving': 132, 'misinformation': 63, 'cryptocurrency': 41, 'forensics': 54, 'Digital assistant': 2, 'Biases': 6, 'humanoid robots': 34, 'digital marketing': 13, 'Privacy': 7, 'smart speaker': 17, 'drug discovery': 8, 'functional genomics': 3, 'infectious diseases': 5, 'gene editing': 20, 'crispr': 4, 'conspiracy theories': 18, 'financial markets': 11, 'Alcohol interfer with recovery and adaptation to training': 32, 'The problem Of muscle hypertrophy: Revisited': 5, 'Occlusion Training for Hypertrophy': 5, 'Hypertrophy after Aerobic Exercise': 3, 'Stretch training induces hypertrophy?': 7, 'Cycle training induces muscle hypertrophy': 7, 'Volume for hypertrophy': 7, 'short versus long inter-set rest intervals on hypertrophy': 4, 'Muscle memory': 9, 'Effects of meal frequency on weight loss': 6, 'Resistance Training is Medicine': 7, 'Benefits of strength training for adolescents': 7, 'Resistance training for muscle function deficits': 6, 'Effect of Repetition Duration on Hypertrophy': 7, 'The cardiovascular system after exercise': 9, '\"Exercise as medicine\" in chronic kidney disease': 5, 'blood flow restriction in individuals with knee pain': 6, 'Prescription of exercise training for hypertensives': 6, 'Effects of intermittent fasting on sport performance': 3, 'Maintaining Physical Performance': 6, 'post-exercise anabolic window': 8, 'No Time to Lift? Designing Time‑Efficient Training Programs': 8, 'How many training per week for hypetrophy?': 6, 'Training Techniques for hypertrophy': 3, 'Mechanisms of Muscle Hypertrophy': 7}\n"
     ]
    }
   ],
   "source": [
    "query_lst=[]\n",
    "for data in file1:\n",
    "    query_lst.append(data['query_text'])\n",
    "print(f\"Data size: {len(file1)}\")\n",
    "query_dict={i:query_lst.count(i) for i in query_lst}\n",
    "print(f\"The number of query text: {len(query_dict)}\")\n",
    "print(f\"The amount of data per query text: {query_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d939dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-955af95913bf>:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df1['source_snt'] = df1['source_snt'].str.replace('.',',')\n"
     ]
    }
   ],
   "source": [
    "df1=pd.DataFrame(file1,dtype='object')\n",
    "df1['source_snt'] = df1['source_snt'].str.replace('.',',')\n",
    "df1['source_snt'] = df1[['source_snt', 'query_text']].agg(' related to '.join, axis=1)+\".\"\n",
    "df2=pd.DataFrame(file2)\n",
    "df_total=pd.concat([df1,df2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8640760d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_snt</th>\n",
       "      <th>simplified_snt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the modern era of automation and robotics, ...</td>\n",
       "      <td>Current academic and industrial research is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the ever increasing number of unmanned ae...</td>\n",
       "      <td>Drones are increasingly used in the civilian a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Due to guidelines set by the governments regar...</td>\n",
       "      <td>Governments set guidelines on the operation ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In an attempt to achieve the above mentioned t...</td>\n",
       "      <td>Researchers propose data-driven solutions allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Derived from the classic image classification ...</td>\n",
       "      <td>The algorithm, based on the Inception model, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>Bodybuilders generally train with moderate loa...</td>\n",
       "      <td>Bodybuilders train with moderate loads and sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Powerlifters, on the other hand, routinely tra...</td>\n",
       "      <td>Powerlifters, on the other hand, train with hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>Although both groups are known to display impr...</td>\n",
       "      <td>Although both groups are known to display impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>It has been shown that many factors mediate th...</td>\n",
       "      <td>It has been shown that many factors mediate th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>Therefore, the purpose of this paper is twofol...</td>\n",
       "      <td>Therefore, the purpose of this paper is to rev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            source_snt  \\\n",
       "0    In the modern era of automation and robotics, ...   \n",
       "1    With the ever increasing number of unmanned ae...   \n",
       "2    Due to guidelines set by the governments regar...   \n",
       "3    In an attempt to achieve the above mentioned t...   \n",
       "4    Derived from the classic image classification ...   \n",
       "..                                                 ...   \n",
       "643  Bodybuilders generally train with moderate loa...   \n",
       "644  Powerlifters, on the other hand, routinely tra...   \n",
       "645  Although both groups are known to display impr...   \n",
       "646  It has been shown that many factors mediate th...   \n",
       "647  Therefore, the purpose of this paper is twofol...   \n",
       "\n",
       "                                        simplified_snt  \n",
       "0    Current academic and industrial research is in...  \n",
       "1    Drones are increasingly used in the civilian a...  \n",
       "2    Governments set guidelines on the operation ce...  \n",
       "3    Researchers propose data-driven solutions allo...  \n",
       "4    The algorithm, based on the Inception model, d...  \n",
       "..                                                 ...  \n",
       "643  Bodybuilders train with moderate loads and sho...  \n",
       "644  Powerlifters, on the other hand, train with hi...  \n",
       "645  Although both groups are known to display impr...  \n",
       "646  It has been shown that many factors mediate th...  \n",
       "647  Therefore, the purpose of this paper is to rev...  \n",
       "\n",
       "[648 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_total[['source_snt','simplified_snt']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018d3543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dataframe': 'Train data', 'max length of source sentence': 71, 'avg length of source sentence': 28.734567901234566}\n",
      "{'Dataframe': 'Train data', 'max length of simplified sentence': 61, 'avg length of simplified sentence': 18.814814814814813}\n"
     ]
    }
   ],
   "source": [
    "def data_info(data,column_name):\n",
    "    max_length=max([len(i.split()) for i in data[column_name]])\n",
    "    average_length=sum([len(i.split()) for i in data[column_name]])/len(data[column_name])\n",
    "    return max_length,average_length\n",
    "\n",
    "max_length,average_length=data_info(df,'source_snt')\n",
    "print({'Dataframe':'Train data','max length of source sentence':max_length,'avg length of source sentence':average_length})\n",
    "\n",
    "max_length,average_length=data_info(df,'simplified_snt')\n",
    "print({'Dataframe':'Train data','max length of simplified sentence':max_length,'avg length of simplified sentence':average_length})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789453b0",
   "metadata": {},
   "source": [
    " ## Add control tokens to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a48a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Crystal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import Levenshtein\n",
    "import spacy\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Lock\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords')\n",
    "import threading\n",
    "from queue import Queue\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6cb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.resources import DUMPS_DIR, WORD_FREQUENCY_FILEPATH, WORD_EMBEDDINGS_NAME, download_glove,COMPLEXITY_MODEL_FILEPATH,GOOGLE_WORD_EMBEDDINGS_FILEPATH\n",
    "from source.helper import tokenize, yield_lines, load_dump, dump, write_lines,save_preprocessor, yield_sentence_pair,generate_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44dfb5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def round(val):\n",
    "    return '%.2f' % val\n",
    "\n",
    "def safe_division(a, b):\n",
    "    return a / b if b else 0\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def is_punctuation(word):\n",
    "    return ''.join([char for char in word if char not in punctuation]) == ''\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ' '.join([word for word in tokenize(text) if not is_punctuation(word)])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w for w in tokenize(text) if w.lower() not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eeea9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def get_spacy_model():\n",
    "    model = 'en_core_web_sm'\n",
    "    if not spacy.util.is_package(model):\n",
    "        spacy.cli.download(model)\n",
    "        spacy.cli.link(model, model, force=True, model_path=spacy.util.get_package_path(model))\n",
    "    return spacy.load(model)\n",
    "\n",
    "@lru_cache(maxsize=10 ** 6)\n",
    "def spacy_process(text):\n",
    "    return get_spacy_model()(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c720bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1024)\n",
    "def get_dependency_tree_depth(sentence):\n",
    "    def tree_height(node):\n",
    "        if len(list(node.children)) == 0:\n",
    "            return 0\n",
    "        return 1 + max(tree_height(child) for child in node.children)\n",
    "\n",
    "    tree_depths = [tree_height(spacy_sentence.root) for spacy_sentence in spacy_process(sentence).sents]\n",
    "    if len(tree_depths) == 0:\n",
    "        return 0\n",
    "    return max(tree_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b04b8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def get_word2rank(vocab_size=np.inf):\n",
    "    model_filepath = DUMPS_DIR / f\"{WORD_EMBEDDINGS_NAME}.pk\"\n",
    "    if model_filepath.exists():\n",
    "        return load_dump(model_filepath)\n",
    "    else:\n",
    "        print(\"Downloading glove.42B.300d ...\")\n",
    "        download_glove(model_name='glove.42B.300d', dest_dir=str(DUMPS_DIR))\n",
    "        print(\"Preprocessing word2rank...\")\n",
    "        DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        WORD_EMBEDDINGS_PATH = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "        lines_generator = yield_lines(WORD_EMBEDDINGS_PATH)\n",
    "        word2rank = {}\n",
    "        # next(lines_generator)\n",
    "        for i, line in enumerate(lines_generator):\n",
    "            if i >= vocab_size: break\n",
    "            word = line.split(' ')[0]\n",
    "            word2rank[word] = i\n",
    "        dump(word2rank, model_filepath)\n",
    "        txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "        zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "        if txt_file.exists(): txt_file.unlink()\n",
    "        if zip_file.exists(): zip_file.unlink()\n",
    "        return word2rank\n",
    "\n",
    "get_word2rank()\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_normalized_rank(word):\n",
    "    max = len(get_word2rank())\n",
    "    rank = get_word2rank().get(word, max)\n",
    "    return np.log(1 + rank) / np.log(1 + max)\n",
    "\n",
    "@lru_cache(maxsize=5000)\n",
    "def get_rank(word):\n",
    "    rank = get_word2rank().get(word, len(get_word2rank()))\n",
    "    return np.log(1 + rank)\n",
    "\n",
    "@lru_cache(maxsize=2048)\n",
    "def get_lexical_complexity_score(sentence):\n",
    "    words = tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "    words = [word for word in words if word in get_word2rank()]  # remove unknown words\n",
    "    if len(words) == 0:\n",
    "        return 1.0\n",
    "#     return np.array([get_normalized_rank(word) for word in words]).mean()\n",
    "    return np.quantile([get_rank(word) for word in words], 0.75)\n",
    "\n",
    "# @lru_cache(maxsize=2048)\n",
    "def get_lexical_complexity_score_batch(simple_sentences,complex_sentences):\n",
    "    scores=[]\n",
    "    for i in range(len(simple_sentences)):\n",
    "        scores.append(float(round(min(safe_division(get_lexical_complexity_score(simple_sentences[i]),\n",
    "                                       get_lexical_complexity_score(complex_sentences[i])), 2))))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b9357b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sentences=['Current academic and industrial research is interested in autonomous vehicles.','Current academic and industrial research is interested in autonomous vehicles.']\n",
    "complex_sentences=['In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.','In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.']\n",
    "get_lexical_complexity_score_batch(simple_sentences,complex_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ff7f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_word_frequency():\n",
    "    model_filepath = DUMPS_DIR / f'{WORD_FREQUENCY_FILEPATH.stem}.pk'\n",
    "    if model_filepath.exists():\n",
    "        return load_dump(model_filepath)\n",
    "    else:\n",
    "        DUMPS_DIR.mkdir(parents=True, exist_ok=True) \n",
    "        word_freq = {}\n",
    "        with open(WORD_FREQUENCY_FILEPATH,'r',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                chunks = line.rstrip().split(' ')\n",
    "                word = chunks[0]\n",
    "                freq = int(chunks[1])\n",
    "                word_freq[word] = freq\n",
    "        dump(word_freq, model_filepath)\n",
    "        return word_freq\n",
    "\n",
    "get_word_frequency()\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_normalized_frequency(word):\n",
    "    max = 179573112 # the 153141437, the max frequency\n",
    "    freq = get_word_frequency().get(word, 0)\n",
    "    return 1.0 - np.log(1 + freq) / np.log(1 + max)\n",
    "\n",
    "@lru_cache(maxsize=2048)\n",
    "def get_complexity_score(sentence):\n",
    "    words = tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "#     words = tokenize(remove_punctuation(sentence))\n",
    "    words = [word for word in words if word in get_word_frequency()]  # remove unknown words\n",
    "    if len(words) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    return np.array([get_normalized_frequency(word.lower()) for word in words]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8c0d9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.837459915682472"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence='Current academic and industrial research is interested in autonomous vehicles.'\n",
    "# sentence='In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.'\n",
    "words = tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "words = [word for word in words if word in get_word2rank()]\n",
    "rank=np.quantile([get_rank(word) for word in words], 0.75)\n",
    "# rank=np.array([get_normalized_rank(word) for word in words]).mean()\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "068a67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatioFeature:\n",
    "    def __init__(self, feature_extractor, target_ratio=0.80):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.target_ratio = str(target_ratio)\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        return f'{self.name}_{self.target_ratio}'\n",
    "\n",
    "    def encode_sentence_pair(self, complex_sentence, simple_sentence):\n",
    "        return f'{self.name}_{self.feature_extractor(complex_sentence, simple_sentence)}', simple_sentence\n",
    "\n",
    "    def decode_sentence(self, encoded_sentence):\n",
    "        return encoded_sentence\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        class_name = self.__class__.__name__.replace('RatioFeature', '')\n",
    "        name = \"\"\n",
    "        for word in re.findall('[A-Z][^A-Z]*', class_name):\n",
    "            if word: name += word[0]\n",
    "        if not name: name = class_name\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0599d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_word_length_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_word_length_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(safe_division(len(tokenize(simple_sentence)), len(tokenize(complex_sentence))))\n",
    "\n",
    "class CharRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_char_length_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_char_length_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(safe_division(len(simple_sentence), len(complex_sentence)))\n",
    "\n",
    "\n",
    "class LevenshteinRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_levenshtein_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_levenshtein_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(Levenshtein.ratio(complex_sentence, simple_sentence))\n",
    "\n",
    "\n",
    "class WordRankRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_word_rank_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_word_rank_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(min(safe_division(self.get_lexical_complexity_score(simple_sentence),\n",
    "                                       self.get_lexical_complexity_score(complex_sentence)), 2))\n",
    "\n",
    "    def get_lexical_complexity_score(self, sentence):\n",
    "        words = tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "        words = [word for word in words if word in get_word2rank()]\n",
    "        if len(words) == 0:\n",
    "            return np.log(1 + len(get_word2rank()))\n",
    "        return np.quantile([self.get_rank(word) for word in words], 0.75)\n",
    "\n",
    "    @lru_cache(maxsize=5000)\n",
    "    def get_rank(self, word):\n",
    "        rank = get_word2rank().get(word, len(get_word2rank()))\n",
    "        return np.log(1 + rank)\n",
    "\n",
    "\n",
    "class DependencyTreeDepthRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_dependency_tree_depth_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_dependency_tree_depth_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(\n",
    "            safe_division(self.get_dependency_tree_depth(simple_sentence),\n",
    "                          self.get_dependency_tree_depth(complex_sentence)))\n",
    "    \n",
    "    @lru_cache(maxsize=1024)\n",
    "    def get_dependency_tree_depth(self, sentence):\n",
    "        def get_subtree_depth(node):\n",
    "            if len(list(node.children)) == 0:\n",
    "                return 0\n",
    "            return 1 + max([get_subtree_depth(child) for child in node.children])\n",
    "\n",
    "        tree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in self.spacy_process(sentence).sents]\n",
    "        if len(tree_depths) == 0:\n",
    "            return 0\n",
    "        return max(tree_depths)\n",
    "    \n",
    "    @lru_cache(maxsize=10 ** 6)\n",
    "    def spacy_process(self, text):\n",
    "        return get_spacy_model()(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03f304f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, features_kwargs=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = self.get_features(features_kwargs)\n",
    "        if features_kwargs:\n",
    "            self.hash = generate_hash(str(features_kwargs).encode())\n",
    "        else:\n",
    "            self.hash = \"no_feature\"\n",
    "\n",
    "    def get_class(self, class_name, *args, **kwargs):\n",
    "        return globals()[class_name](*args, **kwargs)\n",
    "\n",
    "    def get_features(self, feature_kwargs):\n",
    "        features = []\n",
    "        for feature_name, kwargs in feature_kwargs.items():\n",
    "            features.append(self.get_class(feature_name, **kwargs))\n",
    "        return features\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        if self.features:\n",
    "            line = ''\n",
    "            for feature in self.features:\n",
    "                line += feature.encode_sentence(sentence) + ' '\n",
    "            line += sentence\n",
    "            return line.rstrip()\n",
    "        else:\n",
    "            return sentence\n",
    "\n",
    "    def encode_sentence_pair(self, complex_sentence, simple_sentence):\n",
    "        if self.features:\n",
    "            line = ''\n",
    "            for feature in self.features:\n",
    "                processed_complex, _ = feature.encode_sentence_pair(complex_sentence, simple_sentence)\n",
    "                line += processed_complex + ' '\n",
    "            line += complex_sentence\n",
    "            return line.rstrip()\n",
    "\n",
    "        else:\n",
    "            return complex_sentence\n",
    "\n",
    "    def decode_sentence(self, encoded_sentence):\n",
    "        for feature in self.features:\n",
    "            decoded_sentence = feature.decode_sentence(encoded_sentence)\n",
    "        return decoded_sentence\n",
    "\n",
    "    def encode_file(self, input_filepath, output_filepath):\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            for line in yield_lines(input_filepath):\n",
    "                f.write(self.encode_sentence(line) + '\\n')\n",
    "\n",
    "    def decode_file(self, input_filepath, output_filepath):\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            for line in yield_lines(input_filepath):\n",
    "                f.write(self.decode_sentence(line) + '\\n')\n",
    "\n",
    "    def encode_dataframe(self,dataset):\n",
    "        processed_complex_sentences = []\n",
    "        for complex_sentence, simple_sentence in tqdm(zip(dataset['source_snt'], dataset['simplified_snt']),total=len(dataset)):\n",
    "            processed_complex_sentence = self.encode_sentence_pair(complex_sentence, simple_sentence)\n",
    "            processed_complex_sentences.append(processed_complex_sentence)\n",
    "        return processed_complex_sentences\n",
    "\n",
    "    def preprocess_dataset(self, dataset):\n",
    "        new_df=dataset.copy()\n",
    "        new_df['source_snt']= self.encode_dataframe(dataset)\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b2e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_kwargs = {\n",
    "        'WordRatioFeature': {'target_ratio': '1.05'},\n",
    "        'CharRatioFeature': {'target_ratio': '0.95'},\n",
    "        'LevenshteinRatioFeature': {'target_ratio': '0.75'},\n",
    "        'WordRankRatioFeature': {'target_ratio': '0.85'},\n",
    "        'DependencyTreeDepthRatioFeature': {'target_ratio': '0.85'}\n",
    "    }\n",
    "# features_kwargs = {}\n",
    "preprocessor = Preprocessor(features_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9743cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W_1.05 C_0.95 L_0.75 WR_0.85 DTD_0.85 In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(features_kwargs)\n",
    "# preprocessor.encode_sentence_pair('In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.','Current academic and industrial research is interested in autonomous vehicles.')\n",
    "preprocessor.encode_sentence('In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "972391dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.88'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sentence='Current academic and industrial research is interested in autonomous vehicles.'\n",
    "complex_sentence='In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.'\n",
    "round(min(safe_division(get_lexical_complexity_score(simple_sentence),\n",
    "                                       get_lexical_complexity_score(complex_sentence)),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80ff86",
   "metadata": {},
   "source": [
    "simple_sentence='Current academic and industrial research is interested in autonomous vehicles.'\n",
    "complex_sentence='In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.'\n",
    "round(min(safe_division(get_complexity_score(simple_sentence),\n",
    "                                       get_complexity_score(complex_sentence)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98f1870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 648/648 [00:15<00:00, 40.53it/s]\n"
     ]
    }
   ],
   "source": [
    "new_df = preprocessor.preprocess_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32b16d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_snt</th>\n",
       "      <th>simplified_snt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W_0.44 C_0.54 L_0.47 WR_0.88 DTD_0.80 In the m...</td>\n",
       "      <td>Current academic and industrial research is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W_0.44 C_0.44 L_0.52 WR_1.00 DTD_0.36 With the...</td>\n",
       "      <td>Drones are increasingly used in the civilian a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W_0.77 C_0.78 L_0.73 WR_0.98 DTD_0.75 Due to g...</td>\n",
       "      <td>Governments set guidelines on the operation ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W_0.51 C_0.61 L_0.61 WR_1.00 DTD_0.58 In an at...</td>\n",
       "      <td>Researchers propose data-driven solutions allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W_0.64 C_0.67 L_0.51 WR_0.96 DTD_0.45 Derived ...</td>\n",
       "      <td>The algorithm, based on the Inception model, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>W_0.38 C_0.37 L_0.54 WR_1.00 DTD_0.50 Bodybuil...</td>\n",
       "      <td>Bodybuilders train with moderate loads and sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>W_0.62 C_0.57 L_0.73 WR_0.98 DTD_0.67 Powerlif...</td>\n",
       "      <td>Powerlifters, on the other hand, train with hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>W_0.79 C_0.77 L_0.81 WR_0.95 DTD_1.00 Although...</td>\n",
       "      <td>Although both groups are known to display impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>W_0.85 C_0.75 L_0.78 WR_0.93 DTD_0.75 It has b...</td>\n",
       "      <td>It has been shown that many factors mediate th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>W_0.72 C_0.74 L_0.82 WR_0.98 DTD_0.93 Therefor...</td>\n",
       "      <td>Therefore, the purpose of this paper is to rev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            source_snt  \\\n",
       "0    W_0.44 C_0.54 L_0.47 WR_0.88 DTD_0.80 In the m...   \n",
       "1    W_0.44 C_0.44 L_0.52 WR_1.00 DTD_0.36 With the...   \n",
       "2    W_0.77 C_0.78 L_0.73 WR_0.98 DTD_0.75 Due to g...   \n",
       "3    W_0.51 C_0.61 L_0.61 WR_1.00 DTD_0.58 In an at...   \n",
       "4    W_0.64 C_0.67 L_0.51 WR_0.96 DTD_0.45 Derived ...   \n",
       "..                                                 ...   \n",
       "643  W_0.38 C_0.37 L_0.54 WR_1.00 DTD_0.50 Bodybuil...   \n",
       "644  W_0.62 C_0.57 L_0.73 WR_0.98 DTD_0.67 Powerlif...   \n",
       "645  W_0.79 C_0.77 L_0.81 WR_0.95 DTD_1.00 Although...   \n",
       "646  W_0.85 C_0.75 L_0.78 WR_0.93 DTD_0.75 It has b...   \n",
       "647  W_0.72 C_0.74 L_0.82 WR_0.98 DTD_0.93 Therefor...   \n",
       "\n",
       "                                        simplified_snt  \n",
       "0    Current academic and industrial research is in...  \n",
       "1    Drones are increasingly used in the civilian a...  \n",
       "2    Governments set guidelines on the operation ce...  \n",
       "3    Researchers propose data-driven solutions allo...  \n",
       "4    The algorithm, based on the Inception model, d...  \n",
       "..                                                 ...  \n",
       "643  Bodybuilders train with moderate loads and sho...  \n",
       "644  Powerlifters, on the other hand, train with hi...  \n",
       "645  Although both groups are known to display impr...  \n",
       "646  It has been shown that many factors mediate th...  \n",
       "647  Therefore, the purpose of this paper is to rev...  \n",
       "\n",
       "[648 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438dadc",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "81642038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_l=load_data('test/simpletext-task3-test-large.json')\n",
    "# test_m=load_data('test/simpletext-task3-test-medium.json')\n",
    "test_s=load_data('test/simpletext-task3-test-small.json')\n",
    "def load_test_set(file):\n",
    "    dataset=pd.DataFrame(file,dtype='object')\n",
    "    for index, row in dataset.iterrows():\n",
    "        if not pd.isnull(row['query_text']):\n",
    "            dataset.at[index, 'source_snt'] = row['source_snt'].replace('.', ',') + ' related to ' + row['query_text'] + '.'\n",
    "    dataset=dataset[['source_snt']]\n",
    "    return dataset\n",
    "test_small=load_test_set(test_s)\n",
    "# test_medium=load_test_set(test_m)\n",
    "# test_large=load_test_set(test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ea1d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a rich console logger\n",
    "console = Console(record=True)\n",
    "\n",
    "# to display dataframe in ASCII format\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        Column(\"source_text\", justify=\"left\"),\n",
    "        Column(\"target_text\", justify=\"left\"),\n",
    "        title=\"Sample Data\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5ef18",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9d0adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "        row_source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = \" \".join(row_source_text.split())\n",
    "        target_text = \" \".join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [target_text],\n",
    "            max_length=self.summ_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_ids = target[\"input_ids\"].squeeze()\n",
    "        target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_text\":row_source_text,\n",
    "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
    "            \"target_mask\": target_mask.to(dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6fc06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataSetClass(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, source_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            source_text (str): column name of source text\n",
    "\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.source_text = self.data[source_text]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.source_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks\"\"\"\n",
    "\n",
    "        source_text = str(self.source_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = \" \".join(source_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c9fd8",
   "metadata": {},
   "source": [
    "## Train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5dfc72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training logger to log training progress\n",
    "def training_logger_init():\n",
    "    training_logger = Table(\n",
    "        Column(\"Epoch\", justify=\"center\"),\n",
    "        Column(\"Steps\", justify=\"center\"),\n",
    "        Column(\"Loss\", justify=\"center\"),\n",
    "        Column(\"ROUGE_Loss\", justify=\"center\"),\n",
    "        Column(\"ROUGE_1\", justify=\"center\"),\n",
    "        Column(\"ROUGE_2\", justify=\"center\"),\n",
    "        Column(\"ROUGE_L\", justify=\"center\"),\n",
    "        Column(\"SARI\", justify=\"center\"),\n",
    "        Column(\"BLEU\", justify=\"center\"),\n",
    "        Column(\"FKGL\", justify=\"center\"),\n",
    "        title=\"Training Status\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "    return training_logger\n",
    "\n",
    "# training logger to log training progress\n",
    "def epoch_training_logger_init():\n",
    "    epoch_training_logger = Table(\n",
    "        Column(\"Epoch\", justify=\"center\"),\n",
    "        Column(\"Train_Loss\", justify=\"center\"),\n",
    "        Column(\"Valid_Loss\", justify=\"center\"),\n",
    "        Column(\"ROUGE_1\", justify=\"center\"),\n",
    "        Column(\"ROUGE_2\", justify=\"center\"),\n",
    "        Column(\"ROUGE_L\", justify=\"center\"),\n",
    "        Column(\"SARI\", justify=\"center\"),\n",
    "        Column(\"BLEU\", justify=\"center\"),\n",
    "        Column(\"FKGL\", justify=\"center\"),\n",
    "        title=\"Training Epoch Status\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "    return epoch_training_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0577a67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7721102818691421"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "from evaluate import load\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_rouge(predicted, target):\n",
    "    rouger = Rouge()\n",
    "    scores = rouger.get_scores(predicted, target, avg=True)\n",
    "    return scores['rouge-1']['f'],scores['rouge-2']['f'],scores['rouge-l']['f']\n",
    "\n",
    "def compute_bleu(predicted, target):\n",
    "    bleu_scores = []\n",
    "    for i in range(len(predicted)):\n",
    "        bleu_scores.append(sentence_bleu([target[i]],predicted[i], weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "def compute_sari(sources, predicted, target):\n",
    "    sari = load(\"sari\")\n",
    "    sari_scores=sari.compute(sources=sources, predictions=predicted, references=[[i] for i in target])\n",
    "    return sari_scores['sari']\n",
    "\n",
    "sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "predictions=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "references=[\"About 95 species are currently known.\",\"About 95 species are currently known.\"]\n",
    "compute_bleu(predictions,references)\n",
    "# compute_sari(sources,predictions,references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0556cc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.34588841607616"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation.sari import corpus_sari\n",
    "from evaluation.bleu import corpus_bleu\n",
    "from evaluation.fkgl import corpus_fkgl\n",
    "sources=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "predictions=[\"About 95 species are currently accepted.\",\"About 95 species are currently accepted.\"]\n",
    "references=[\"About 95 species are currently known.\",\"About 95 species are currently known.\"]\n",
    "corpus_sari(sources,predictions,[references],lowercase=True)\n",
    "corpus_bleu(predictions,[references],lowercase=True)\n",
    "# corpus_fkgl(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3053e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer,model_params):\n",
    "    model.train()\n",
    "    total_loss=[]\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "        target_mask = data[\"target_mask\"].to(device, dtype=torch.long)[:, 1:]\n",
    "        source_text = data[\"source_text\"]\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "            decoder_attention_mask=target_mask,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "                  input_ids = ids,\n",
    "                  attention_mask = mask, \n",
    "                  max_length=model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "                  num_beams=model_params[\"NUM_BEAMS\"],\n",
    "                  repetition_penalty=model_params[\"REPETITION_PENALTY\"], \n",
    "                  length_penalty=model_params[\"LENGTH_PENALTY\"], \n",
    "                  early_stopping=True,\n",
    "                  do_sample=False,\n",
    "                  temperature=0.25,\n",
    "                  top_k=120,\n",
    "                  top_p=0.98,\n",
    "                  )\n",
    "\n",
    "        # calculate rouge score\n",
    "        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "        target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "        rouge1,rouge2,rougeL = calculate_rouge(preds, target)\n",
    "#         bleu_score = compute_bleu(preds, target)\n",
    "#         sari_score = compute_sari(source_text,preds, target)\n",
    "        sari_score=corpus_sari(source_text,preds, [target],lowercase=False)\n",
    "        bleu_score=corpus_bleu(preds, [target],lowercase=False)\n",
    "        fkgl_score=corpus_fkgl(target)\n",
    "\n",
    "        # add rouge loss to total loss\n",
    "        rouge_loss = 1.0 - np.mean([rouge1,rouge2,rougeL])\n",
    "        complexity_score = get_lexical_complexity_score_batch(target,preds)#higher is better\n",
    "        \n",
    "        lambda_ = 0.7\n",
    "        loss = lambda_ * loss + (1-lambda_)*(1-complexity_score)\n",
    "\n",
    "        if _ % 10 == 0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss),str(rouge_loss),str(rouge1),str(rouge2),str(rougeL),str(sari_score),str(bleu_score),str(fkgl_score))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         (loss + rouge_loss).backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss)\n",
    "    average_loss=sum(total_loss)/len(total_loss)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bebc99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(tokenizer, model, device, loader,model_params):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    total_loss=[]\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    bleu_scores = []\n",
    "    sari_scores = []\n",
    "    fkgl_scores = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "            y_ids = y[:, :-1].contiguous()\n",
    "            lm_labels = y[:, 1:].clone().detach()\n",
    "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "            target_mask = data[\"target_mask\"].to(device, dtype=torch.long)[:, 1:]\n",
    "            source_text = data[\"source_text\"]\n",
    "            \n",
    "            outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "            decoder_attention_mask=target_mask,\n",
    "            )\n",
    "            loss = outputs[0]\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                  input_ids = ids,\n",
    "                  attention_mask = mask, \n",
    "                  max_length=model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "                  num_beams=model_params[\"NUM_BEAMS\"],\n",
    "                  repetition_penalty=model_params[\"REPETITION_PENALTY\"], \n",
    "                  length_penalty=model_params[\"LENGTH_PENALTY\"], \n",
    "                  early_stopping=True,\n",
    "                  do_sample=False,\n",
    "                  temperature=0.25,\n",
    "                  top_k=120,\n",
    "                  top_p=0.98,\n",
    "                  )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            rouge1,rouge2,rougeL = calculate_rouge(preds, target)\n",
    "            sari_score=corpus_sari(source_text,preds, [target],lowercase=False)\n",
    "            bleu_score=corpus_bleu(preds, [target],lowercase=False)\n",
    "            fkgl_score=corpus_fkgl(target)\n",
    "#             bleu_score = compute_bleu(preds, target)\n",
    "#             sari_score = compute_sari(source_text,preds, target)\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            \n",
    "            total_loss.append(loss)\n",
    "            rouge1_scores.append(rouge1)\n",
    "            rouge2_scores.append(rouge2)\n",
    "            rougeL_scores.append(rougeL)\n",
    "            sari_scores.append(sari_score)\n",
    "            bleu_scores.append(bleu_score)\n",
    "            fkgl_scores.append(fkgl_score)\n",
    "            \n",
    "    average_loss=sum(total_loss)/len(total_loss)\n",
    "    average_rouge1=sum(rouge1_scores)/len(rouge1_scores)\n",
    "    average_rouge2=sum(rouge2_scores)/len(rouge2_scores)\n",
    "    average_rougeL=sum(rougeL_scores)/len(rougeL_scores)\n",
    "    average_bleu=sum(bleu_scores)/len(bleu_scores)\n",
    "    average_sari=sum(sari_scores)/len(sari_scores)\n",
    "    average_fkgl=sum(fkgl_scores)/len(fkgl_scores)\n",
    "    return predictions, actuals,average_loss, average_rouge1,average_rouge2,average_rougeL,average_bleu,average_sari,average_fkgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e885397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(tokenizer, model, device, loader,model_params):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to test model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "                num_beams=model_params[\"NUM_BEAMS\"],\n",
    "                repetition_penalty=model_params[\"REPETITION_PENALTY\"], \n",
    "                length_penalty=model_params[\"LENGTH_PENALTY\"], \n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "                temperature=0.25,\n",
    "                top_k=120,\n",
    "                top_p=0.98,\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            if _%10==0:\n",
    "                console.print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea4f1b",
   "metadata": {},
   "source": [
    "## ModelTrainer and ModelTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c537118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelTrainer(\n",
    "    dataframe, source_text, target_text, model,tokenizer,model_params, output_dir\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "    model = model.to(device)\n",
    "\n",
    "     # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text, target_text]]\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "    train_size = 0.8\n",
    "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "    train_dataset[\"source_snt\"] = \"simplify: \" + train_dataset[\"source_snt\"]\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    \n",
    "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    if model_params[\"CONTROL_TOKENS\"]:\n",
    "        val_dataset[\"source_snt\"]=[preprocessor.encode_sentence(i[38:]) for i in val_dataset[\"source_snt\"]]\n",
    "    val_dataset[\"source_snt\"] = \"simplify: \" + val_dataset[\"source_snt\"]\n",
    "    \n",
    "    display_df(train_dataset.head(2))\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"VALID Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = TrainDataSetClass(\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    val_set = TrainDataSetClass(\n",
    "        val_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        train_loss =train(epoch, tokenizer, model, device, training_loader, optimizer,model_params)\n",
    "        predictions, actuals, valid_loss, average_rouge1,average_rouge2,average_rougeL,average_bleu,average_sari,average_fkgl = validate(tokenizer, model, device, val_loader,model_params)\n",
    "        epoch_training_logger.add_row(str(epoch), str(train_loss), str(valid_loss), str(average_rouge1),str(average_rouge2),str(average_rougeL),str(average_sari),str(average_bleu),str(average_fkgl))\n",
    "        console.print(epoch_training_logger)\n",
    "        \n",
    "    final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "    final_df.to_csv(os.path.join(output_dir, \"valid_predictions.csv\"))\n",
    "\n",
    "    console.log(f\"[Saving Model]...\\n\")\n",
    "    # Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
    "\n",
    "    console.print(\n",
    "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
    "    )\n",
    "    console.print(\n",
    "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'valid_predictions.csv')}\\n\"\"\"\n",
    "    )\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8da1d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelTest(\n",
    "   test,size,source_text,model,tokenizer,model_params, output_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    T5 test\n",
    "\n",
    "    \"\"\"        \n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Testing {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading {size} test data...\\n\")\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    test = test.reset_index(drop=True)\n",
    "    console.print(f\"Test {size} Dataset: {test.shape}\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    test_set = TestDataSetClass(\n",
    "        test,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    test_params = {\n",
    "        \"batch_size\": model_params[\"TEST_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "    # Testing loop\n",
    "    console.log(f\"[Initiating Testing]...\\n\")\n",
    "    predictions = testing(tokenizer, model, device, test_loader,model_params)\n",
    "    final_df = pd.DataFrame({\"simplified_snt\": predictions})\n",
    "    final_df.to_csv(os.path.join(output_dir, f\"test_{size}_predictions.csv\"))\n",
    "\n",
    "    console.log(f\"[Testing Completed.]\\n\")\n",
    "    console.print(\n",
    "        f\"\"\"[Test] Generation on Test data saved @ {os.path.join(output_dir,f'test_{size}_predictions.csv')}\\n\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea1f921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_set,model,tokenizer,size,output_dir):\n",
    "    test_set=test_set.copy()\n",
    "    if model_params[\"CONTROL_TOKENS\"]:\n",
    "        test_set[\"source_snt\"]=[preprocessor.encode_sentence(i) for i in test_set[\"source_snt\"]]\n",
    "    test_set[\"source_snt\"] = \"simplify: \" + test_set[\"source_snt\"]\n",
    "    ModelTest(\n",
    "        test=test_set,\n",
    "        source_text=\"source_snt\",\n",
    "        size=size,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        model_params=model_params,\n",
    "        output_dir=output_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a400a",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2048f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(output_dir,model_class,tokenizer_class,model_name):\n",
    "\n",
    "    # 获取tokenizer和模型配置信息\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    model_config = model_class.config_class.from_pretrained(model_name)\n",
    "\n",
    "    # 拼接完整路径\n",
    "    model_path = os.path.join(output_dir, \"model_files\")\n",
    "\n",
    "    # 加载模型\n",
    "    model = model_class.from_pretrained(model_path, config=model_config)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_summary(text,model,tokenizer):\n",
    "    if model_params[\"CONTROL_TOKENS\"]:\n",
    "        text=preprocessor.encode_sentence(text)\n",
    "    input_ids = tokenizer.encode(\"simplify: \"+text, return_tensors='pt', \n",
    "                                    max_length=tokenizer.model_max_length, \n",
    "                                    truncation=True)\n",
    "    summary_ids = model.generate(input_ids, \n",
    "                                 max_length=model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "                                 num_beams=model_params[\"NUM_BEAMS\"],\n",
    "                                 repetition_penalty=model_params[\"REPETITION_PENALTY\"], \n",
    "                                 length_penalty=model_params[\"LENGTH_PENALTY\"],\n",
    "                                 do_sample=False,\n",
    "                                 temperature=0.25,\n",
    "                                 top_k=120,\n",
    "                                 top_p=0.98,)\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c666b",
   "metadata": {},
   "source": [
    "## T5 model with control tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2e74b",
   "metadata": {},
   "source": [
    "### T5 Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f78b2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03:15:25] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading t5-base<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-35-ea8cec386a2b&gt;:17</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03:15:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading t5-base\u001b[33m...\u001b[0m                                           \u001b[2m<ipython-input-35-ea8cec386a2b>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m17\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-35-ea8cec386a2b&gt;:21</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                               \u001b[2m<ipython-input-35-ea8cec386a2b>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m21\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Sample Data                                                    </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">source_text                                             </span>|<span style=\"font-weight: bold\"> target_text                                            </span>|\n",
       "|--------------------------------------------------------+--------------------------------------------------------|\n",
       "|simplify: W_0.67 C_0.64 L_0.59 WR_0.97 DTD_0.64         | Analysis of studies with different training volumes    |\n",
       "|Meta-regression analysis of non-volume-equated studies  | showed better results for higher frequencies, although |\n",
       "|showed a significant effect favoring higher             | the difference between frequencies of 1 and 3+ days per|\n",
       "|frequencies, although the overall difference in         | week was small.                                        |\n",
       "|magnitude of effect between frequencies of 1 and 3+     |                                                        |\n",
       "|days per week was modest, related to How many training  |                                                        |\n",
       "|per week for hypetrophy?.                               |                                                        |\n",
       "|simplify: W_0.78 C_0.76 L_0.86 WR_1.06 DTD_0.70 Four    | Four major capabilities were identified, each of which |\n",
       "|major capabilities were identified, each of which       | evolves as a result of using the tools.                |\n",
       "|evolves as a result of using the tools, related to      |                                                        |\n",
       "|digital marketing.                                      |                                                        |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                    Sample Data                                                    \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1msource_text                                            \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mtarget_text                                            \u001b[0m|\n",
       "|--------------------------------------------------------+--------------------------------------------------------|\n",
       "|simplify: W_0.67 C_0.64 L_0.59 WR_0.97 DTD_0.64         | Analysis of studies with different training volumes    |\n",
       "|Meta-regression analysis of non-volume-equated studies  | showed better results for higher frequencies, although |\n",
       "|showed a significant effect favoring higher             | the difference between frequencies of 1 and 3+ days per|\n",
       "|frequencies, although the overall difference in         | week was small.                                        |\n",
       "|magnitude of effect between frequencies of 1 and 3+     |                                                        |\n",
       "|days per week was modest, related to How many training  |                                                        |\n",
       "|per week for hypetrophy?.                               |                                                        |\n",
       "|simplify: W_0.78 C_0.76 L_0.86 WR_1.06 DTD_0.70 Four    | Four major capabilities were identified, each of which |\n",
       "|major capabilities were identified, each of which       | evolves as a result of using the tools.                |\n",
       "|evolves as a result of using the tools, related to      |                                                        |\n",
       "|digital marketing.                                      |                                                        |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">648</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m648\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">518</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m518\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">VALID Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "VALID Dataset: \u001b[1m(\u001b[0m\u001b[1;36m130\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-35-ea8cec386a2b&gt;:85</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                           \u001b[2m<ipython-input-35-ea8cec386a2b>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m85\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">   Loss    </span>|<span style=\"font-weight: bold\"> ROUGE_Lo… </span>|<span style=\"font-weight: bold\">  ROUGE_1   </span>|<span style=\"font-weight: bold\">  ROUGE_2  </span>|<span style=\"font-weight: bold\">  ROUGE_L   </span>|<span style=\"font-weight: bold\">   SARI    </span>|<span style=\"font-weight: bold\">    BLEU    </span>|<span style=\"font-weight: bold\">    FKGL   </span>|\n",
       "|------+-------+-----------+-----------+------------+-----------+------------+-----------+------------+-----------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.5913184… | 0.463468… | 0.5700724… | 39.54648… | 35.011813… | 9.3065788…|\n",
       "|      |       | grad_fn=… |           |            |           |            |           |            |           |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  Loss   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Lo…\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_1  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_2 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_L  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  SARI   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   BLEU   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   FKGL   \u001b[0m|\n",
       "|------+-------+-----------+-----------+------------+-----------+------------+-----------+------------+-----------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.5913184… | 0.463468… | 0.5700724… | 39.54648… | 35.011813… | 9.3065788…|\n",
       "|      |       | grad_fn=… |           |            |           |            |           |            |           |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">   Loss    </span>|<span style=\"font-weight: bold\"> ROUGE_Lo… </span>|<span style=\"font-weight: bold\">  ROUGE_1  </span>|<span style=\"font-weight: bold\">  ROUGE_2   </span>|<span style=\"font-weight: bold\">  ROUGE_L  </span>|<span style=\"font-weight: bold\">    SARI    </span>|<span style=\"font-weight: bold\">   BLEU    </span>|<span style=\"font-weight: bold\">    FKGL    </span>|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  Loss   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Lo…\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_1 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_L \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   SARI   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  BLEU   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   FKGL    \u001b[0m|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">   Loss    </span>|<span style=\"font-weight: bold\"> ROUGE_Lo… </span>|<span style=\"font-weight: bold\">  ROUGE_1  </span>|<span style=\"font-weight: bold\">  ROUGE_2   </span>|<span style=\"font-weight: bold\">  ROUGE_L  </span>|<span style=\"font-weight: bold\">    SARI    </span>|<span style=\"font-weight: bold\">   BLEU    </span>|<span style=\"font-weight: bold\">    FKGL    </span>|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  Loss   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Lo…\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_1 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_L \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   SARI   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  BLEU   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   FKGL    \u001b[0m|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">   Loss    </span>|<span style=\"font-weight: bold\"> ROUGE_Lo… </span>|<span style=\"font-weight: bold\">  ROUGE_1  </span>|<span style=\"font-weight: bold\">  ROUGE_2   </span>|<span style=\"font-weight: bold\">  ROUGE_L  </span>|<span style=\"font-weight: bold\">    SARI    </span>|<span style=\"font-weight: bold\">   BLEU    </span>|<span style=\"font-weight: bold\">    FKGL    </span>|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  30   | tensor(2… | 0.495656… | 0.564529… | 0.4078603… | 0.540639… | 29.890770… | 26.68983… | 10.5117886…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  Loss   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Lo…\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_1 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_L \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   SARI   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  BLEU   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   FKGL    \u001b[0m|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  30   | tensor(2… | 0.495656… | 0.564529… | 0.4078603… | 0.540639… | 29.890770… | 26.68983… | 10.5117886…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">   Loss    </span>|<span style=\"font-weight: bold\"> ROUGE_Lo… </span>|<span style=\"font-weight: bold\">  ROUGE_1  </span>|<span style=\"font-weight: bold\">  ROUGE_2   </span>|<span style=\"font-weight: bold\">  ROUGE_L  </span>|<span style=\"font-weight: bold\">    SARI    </span>|<span style=\"font-weight: bold\">   BLEU    </span>|<span style=\"font-weight: bold\">    FKGL    </span>|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  30   | tensor(2… | 0.495656… | 0.564529… | 0.4078603… | 0.540639… | 29.890770… | 26.68983… | 10.5117886…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  40   | tensor(1… | 0.501994… | 0.544505… | 0.4102150… | 0.539296… | 32.998476… | 29.95618… | 11.9783227…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  Loss   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Lo…\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_1 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_L \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   SARI   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  BLEU   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   FKGL    \u001b[0m|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  30   | tensor(2… | 0.495656… | 0.564529… | 0.4078603… | 0.540639… | 29.890770… | 26.68983… | 10.5117886…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  40   | tensor(1… | 0.501994… | 0.544505… | 0.4102150… | 0.539296… | 32.998476… | 29.95618… | 11.9783227…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">   Loss    </span>|<span style=\"font-weight: bold\"> ROUGE_Lo… </span>|<span style=\"font-weight: bold\">  ROUGE_1  </span>|<span style=\"font-weight: bold\">  ROUGE_2   </span>|<span style=\"font-weight: bold\">  ROUGE_L  </span>|<span style=\"font-weight: bold\">    SARI    </span>|<span style=\"font-weight: bold\">   BLEU    </span>|<span style=\"font-weight: bold\">    FKGL    </span>|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  30   | tensor(2… | 0.495656… | 0.564529… | 0.4078603… | 0.540639… | 29.890770… | 26.68983… | 10.5117886…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  40   | tensor(1… | 0.501994… | 0.544505… | 0.4102150… | 0.539296… | 32.998476… | 29.95618… | 11.9783227…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  50   | tensor(2… | 0.594907… | 0.464073… | 0.3162082… | 0.434997… | 26.261312… | 21.22109… | 12.5685644…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  Loss   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Lo…\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_1 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_L \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   SARI   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  BLEU   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   FKGL    \u001b[0m|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  30   | tensor(2… | 0.495656… | 0.564529… | 0.4078603… | 0.540639… | 29.890770… | 26.68983… | 10.5117886…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  40   | tensor(1… | 0.501994… | 0.544505… | 0.4102150… | 0.539296… | 32.998476… | 29.95618… | 11.9783227…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  50   | tensor(2… | 0.594907… | 0.464073… | 0.3162082… | 0.434997… | 26.261312… | 21.22109… | 12.5685644…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">   Loss    </span>|<span style=\"font-weight: bold\"> ROUGE_Lo… </span>|<span style=\"font-weight: bold\">  ROUGE_1  </span>|<span style=\"font-weight: bold\">  ROUGE_2   </span>|<span style=\"font-weight: bold\">  ROUGE_L  </span>|<span style=\"font-weight: bold\">    SARI    </span>|<span style=\"font-weight: bold\">   BLEU    </span>|<span style=\"font-weight: bold\">    FKGL    </span>|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  30   | tensor(2… | 0.495656… | 0.564529… | 0.4078603… | 0.540639… | 29.890770… | 26.68983… | 10.5117886…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  40   | tensor(1… | 0.501994… | 0.544505… | 0.4102150… | 0.539296… | 32.998476… | 29.95618… | 11.9783227…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  50   | tensor(2… | 0.594907… | 0.464073… | 0.3162082… | 0.434997… | 26.261312… | 21.22109… | 12.5685644…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  60   | tensor(2… | 0.510522… | 0.540574… | 0.4085155… | 0.519340… | 31.044107… | 23.72217… | 12.7575000…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  Loss   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Lo…\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_1 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_L \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   SARI   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  BLEU   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   FKGL    \u001b[0m|\n",
       "|------+-------+-----------+-----------+-----------+------------+-----------+------------+-----------+------------|\n",
       "|  0   |   0   | tensor(5… | 0.458380… | 0.591318… | 0.4634683… | 0.570072… | 39.546486… | 35.01181… | 9.30657883…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  10   | tensor(3… | 0.605744… | 0.442860… | 0.3145404… | 0.425366… | 26.621987… | 22.32893… | 12.3731117…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  20   | tensor(3… | 0.503739… | 0.537570… | 0.4136399… | 0.537570… | 35.233533… | 29.36583… | 12.4469982…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  30   | tensor(2… | 0.495656… | 0.564529… | 0.4078603… | 0.540639… | 29.890770… | 26.68983… | 10.5117886…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  40   | tensor(1… | 0.501994… | 0.544505… | 0.4102150… | 0.539296… | 32.998476… | 29.95618… | 11.9783227…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  50   | tensor(2… | 0.594907… | 0.464073… | 0.3162082… | 0.434997… | 26.261312… | 21.22109… | 12.5685644…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "|  0   |  60   | tensor(2… | 0.510522… | 0.540574… | 0.4085155… | 0.519340… | 31.044107… | 23.72217… | 12.7575000…|\n",
       "|      |       | grad_fn=… |           |           |            |           |            |           |            |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                               Training Epoch Status                                               </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Train_Loss </span>|<span style=\"font-weight: bold\"> Valid_Loss </span>|<span style=\"font-weight: bold\">  ROUGE_1   </span>|<span style=\"font-weight: bold\">   ROUGE_2   </span>|<span style=\"font-weight: bold\">  ROUGE_L   </span>|<span style=\"font-weight: bold\">    SARI     </span>|<span style=\"font-weight: bold\">    BLEU    </span>|<span style=\"font-weight: bold\">     FKGL    </span>|\n",
       "|------+------------+------------+------------+-------------+------------+-------------+------------+-------------|\n",
       "|  0   | tensor(2.… | tensor(2.… | 0.5527581… | 0.42934192… | 0.5403082… | 31.1182636… | 31.082029… | 11.89874514…|\n",
       "|      | grad_fn=&lt;… |            |            |             |            |             |            |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                               Training Epoch Status                                               \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mTrain_Loss\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mValid_Loss\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_1  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m ROUGE_L  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   SARI    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   BLEU   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    FKGL    \u001b[0m|\n",
       "|------+------------+------------+------------+-------------+------------+-------------+------------+-------------|\n",
       "|  0   | tensor(2.… | tensor(2.… | 0.5527581… | 0.42934192… | 0.5403082… | 31.1182636… | 31.082029… | 11.89874514…|\n",
       "|      | grad_fn=<… |            |            |             |            |             |            |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04:34:31] </span><span style=\"font-weight: bold\">[</span>Saving Model<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-35-ea8cec386a2b&gt;:96</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04:34:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mSaving Model\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                     \u001b[2m<ipython-input-35-ea8cec386a2b>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m96\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span> Model saved @ T5_outputs\\model_files\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m Model saved @ T5_outputs\\model_files\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Validation<span style=\"font-weight: bold\">]</span> Generation on Validation data saved @ T5_outputs\\valid_predictions.csv\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mValidation\u001b[1m]\u001b[0m Generation on Validation data saved @ T5_outputs\\valid_predictions.csv\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Logs<span style=\"font-weight: bold\">]</span> Logs saved @ T5_outputs\\logs.txt\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mLogs\u001b[1m]\u001b[0m Logs saved @ T5_outputs\\logs.txt\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenzier for encoding the text\n",
    "model_params = {\n",
    "    \"MODEL\": \"t5-base\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TEST_BATCH_SIZE\": 8,  # test batch size\n",
    "    \"TRAIN_EPOCHS\": 5,  # number of training epochs\n",
    "    \"LEARNING_RATE\": 3e-4,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 100,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 75,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "    \"NUM_BEAMS\":8,\n",
    "    \"REPETITION_PENALTY\":2.5, \n",
    "    \"LENGTH_PENALTY\":1, \n",
    "    \"CONTROL_TOKENS\":True,\n",
    "}\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_params[\"MODEL\"])\n",
    "training_logger = training_logger_init()\n",
    "epoch_training_logger=epoch_training_logger_init()\n",
    "\n",
    "# control tokens\n",
    "features_kwargs = {\n",
    "        'WordRatioFeature': {'target_ratio': '1.05'},\n",
    "        'CharRatioFeature': {'target_ratio': '0.95'},\n",
    "        'LevenshteinRatioFeature': {'target_ratio': '0.75'},\n",
    "        'WordRankRatioFeature': {'target_ratio': '0.95'},\n",
    "        'DependencyTreeDepthRatioFeature': {'target_ratio': '0.85'}\n",
    "    }\n",
    "# features_kwargs = {}\n",
    "preprocessor = Preprocessor(features_kwargs)\n",
    "\n",
    "ModelTrainer(\n",
    "    dataframe=new_df,\n",
    "    source_text=\"source_snt\",\n",
    "    target_text=\"simplified_snt\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_params=model_params,\n",
    "    output_dir=\"T5_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46655fc",
   "metadata": {},
   "source": [
    "### T5 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8208414f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:02:36] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Testing t5-small<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-20-d3131938de95&gt;:9</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:02:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Testing t5-small\u001b[33m...\u001b[0m                                           \u001b[2m<ipython-input-20-d3131938de95>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m9\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                       \u001b[2m                                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading small test data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-20-d3131938de95&gt;:13</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading small test data\u001b[33m...\u001b[0m                                    \u001b[2m<ipython-input-20-d3131938de95>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m13\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Test small Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2234</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Test small Dataset: \u001b[1m(\u001b[0m\u001b[1;36m2234\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Testing<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-20-d3131938de95&gt;:37</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Testing\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                               \u001b[2m<ipython-input-20-d3131938de95>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m37\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m10\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m20\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m30\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m40\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m50\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m60\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m70\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m80\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m90\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m110\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m120\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m130\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">140</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m140\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m150\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m160\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">170</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m170\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">180</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m180\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">190</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m190\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m200\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">210</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m210\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">220</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m220\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">230</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m230\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">240</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m240\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">250</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m250\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">260</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m260\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">270</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m270\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:38:01] </span><span style=\"font-weight: bold\">[</span>Testing Completed.<span style=\"font-weight: bold\">]</span>                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-20-d3131938de95&gt;:42</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:38:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mTesting Completed.\u001b[1m]\u001b[0m                                                  \u001b[2m<ipython-input-20-d3131938de95>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m42\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Test<span style=\"font-weight: bold\">]</span> Generation on Test data saved @ T5_outputs/test_small_predictions.csv\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mTest\u001b[1m]\u001b[0m Generation on Test data saved @ T5_outputs/test_small_predictions.csv\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model,tokenizer=load_model(\"T5_outputs\",T5ForConditionalGeneration,T5Tokenizer,\"t5-small\")\n",
    "test(test_small,model,tokenizer,'small',\"T5_outputs\")\n",
    "# test(test_medium,model,tokenizer,'medium',\"T5_outputs\")\n",
    "# test(test_medium,model,tokenizer,'large',\"T5_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6293bf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": W_1.05 C_0.95 L_0.75 WR_0.95 DTD_0.85 In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.\n"
     ]
    }
   ],
   "source": [
    "model,tokenizer=load_model(\"T5_outputs\",T5ForConditionalGeneration,T5TokenizerFast,\"t5-base\")\n",
    "text='In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.'\n",
    "generate_summary(text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a3a0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_format(size, run):\n",
    "    #load test results\n",
    "    file=load_data(f'test/simpletext-task3-test-{size}.json')\n",
    "\n",
    "    df1=pd.read_csv(f\"T5_outputs/test_{size}_predictions.csv\")\n",
    "    df2=pd.DataFrame(file)\n",
    "    snt_id=df2[['snt_id']]\n",
    "    \n",
    "    df1 = df1.drop(df1.columns[0], axis=1)\n",
    "    df1.insert(0, 'run_id', f'QH_task_3_run{run}')\n",
    "    df1.insert(1, 'manual', 0)\n",
    "    df1.insert(2, 'snt_id', snt_id)\n",
    "\n",
    "    df1.to_json(f'{size}_pre_with_id.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0a5ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 1\n",
    "for size in ['small']:\n",
    "    output_format(size, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eefb6a1",
   "metadata": {},
   "source": [
    "## T5 model without control tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenzier for encoding the text\n",
    "model_params = {\n",
    "    \"MODEL\": \"t5-small\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TEST_BATCH_SIZE\": 8,  # test batch size\n",
    "    \"TRAIN_EPOCHS\": 3,  # number of training epochs\n",
    "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 70,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 55,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "    \"NUM_BEAMS\":8,\n",
    "    \"REPETITION_PENALTY\":2.5, \n",
    "    \"LENGTH_PENALTY\":1, \n",
    "    \"CONTROL_TOKENS\":False,\n",
    "}\n",
    "\n",
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_params[\"MODEL\"])\n",
    "training_logger = training_logger_init()\n",
    "epoch_training_logger=epoch_training_logger_init()\n",
    "\n",
    "# features_kwargs = {}\n",
    "preprocessor = Preprocessor(features_kwargs)\n",
    "\n",
    "ModelTrainer(\n",
    "    dataframe=df,\n",
    "    source_text=\"source_snt\",\n",
    "    target_text=\"simplified_snt\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_params=model_params,\n",
    "    output_dir=\"T5_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1eeba",
   "metadata": {},
   "source": [
    "## BART model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dc5e7",
   "metadata": {},
   "source": [
    "### BART training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46905214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[06:54:22] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading facebook/bart-large-cnn<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-deeb963aca67&gt;:17</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[06:54:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading facebook/bart-large-cnn\u001b[33m...\u001b[0m                           \u001b[2m<ipython-input-27-deeb963aca67>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m17\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-deeb963aca67&gt;:21</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                               \u001b[2m<ipython-input-27-deeb963aca67>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m21\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Sample Data                                                    </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">                      source_text                       </span>|<span style=\"font-weight: bold\">                       target_text                      </span>|\n",
       "|--------------------------------------------------------+--------------------------------------------------------|\n",
       "|   &lt;s&gt;In the modern era of automation and robotics,     | Current academic and industrial research is interested |\n",
       "|autonomous vehicles are currently the focus of academic |                 in autonomous vehicles.                |\n",
       "|    and industrial research, related to drones.&lt;/s&gt;     |                                                        |\n",
       "| &lt;s&gt;With the ever increasing number of unmanned aerial  |    Drones are increasingly used in the civilian and    |\n",
       "|vehicles getting involved in activities in the civilian |      commercial domain and need to be autonomous.      |\n",
       "| and commercial domain, there is an increased need for  |                                                        |\n",
       "| autonomy in these systems too, related to drones.&lt;/s&gt;  |                                                        |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                    Sample Data                                                    \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1m                      source_text                      \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                      target_text                      \u001b[0m|\n",
       "|--------------------------------------------------------+--------------------------------------------------------|\n",
       "|   <s>In the modern era of automation and robotics,     | Current academic and industrial research is interested |\n",
       "|autonomous vehicles are currently the focus of academic |                 in autonomous vehicles.                |\n",
       "|    and industrial research, related to drones.</s>     |                                                        |\n",
       "| <s>With the ever increasing number of unmanned aerial  |    Drones are increasingly used in the civilian and    |\n",
       "|vehicles getting involved in activities in the civilian |      commercial domain and need to be autonomous.      |\n",
       "| and commercial domain, there is an increased need for  |                                                        |\n",
       "| autonomy in these systems too, related to drones.</s>  |                                                        |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">648</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m648\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">518</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m518\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m130\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-deeb963aca67&gt;:79</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                           \u001b[2m<ipython-input-27-deeb963aca67>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m79\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">    Loss     </span>|<span style=\"font-weight: bold\"> ROUGE_Loss  </span>|<span style=\"font-weight: bold\">   ROUGE_1   </span>|<span style=\"font-weight: bold\">   ROUGE_2   </span>|<span style=\"font-weight: bold\">   ROUGE_L   </span>|<span style=\"font-weight: bold\">     SARI     </span>|<span style=\"font-weight: bold\">     BLEU    </span>|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   Loss    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Loss \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_1  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_L  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    SARI    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    BLEU    \u001b[0m|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">    Loss     </span>|<span style=\"font-weight: bold\"> ROUGE_Loss  </span>|<span style=\"font-weight: bold\">   ROUGE_1   </span>|<span style=\"font-weight: bold\">   ROUGE_2   </span>|<span style=\"font-weight: bold\">   ROUGE_L   </span>|<span style=\"font-weight: bold\">     SARI     </span>|<span style=\"font-weight: bold\">     BLEU    </span>|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   Loss    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Loss \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_1  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_L  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    SARI    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    BLEU    \u001b[0m|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">    Loss     </span>|<span style=\"font-weight: bold\"> ROUGE_Loss  </span>|<span style=\"font-weight: bold\">   ROUGE_1   </span>|<span style=\"font-weight: bold\">   ROUGE_2   </span>|<span style=\"font-weight: bold\">   ROUGE_L   </span>|<span style=\"font-weight: bold\">     SARI     </span>|<span style=\"font-weight: bold\">     BLEU    </span>|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   Loss    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Loss \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_1  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_L  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    SARI    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    BLEU    \u001b[0m|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">    Loss     </span>|<span style=\"font-weight: bold\"> ROUGE_Loss  </span>|<span style=\"font-weight: bold\">   ROUGE_1   </span>|<span style=\"font-weight: bold\">   ROUGE_2   </span>|<span style=\"font-weight: bold\">   ROUGE_L   </span>|<span style=\"font-weight: bold\">     SARI     </span>|<span style=\"font-weight: bold\">     BLEU    </span>|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  30   | tensor(1.1… | 0.56948430… | 0.49168430… | 0.32107311… | 0.47878965… | 50.69493337… | 0.354248958…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   Loss    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Loss \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_1  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_L  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    SARI    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    BLEU    \u001b[0m|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  30   | tensor(1.1… | 0.56948430… | 0.49168430… | 0.32107311… | 0.47878965… | 50.69493337… | 0.354248958…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">    Loss     </span>|<span style=\"font-weight: bold\"> ROUGE_Loss  </span>|<span style=\"font-weight: bold\">   ROUGE_1   </span>|<span style=\"font-weight: bold\">   ROUGE_2   </span>|<span style=\"font-weight: bold\">   ROUGE_L   </span>|<span style=\"font-weight: bold\">     SARI     </span>|<span style=\"font-weight: bold\">     BLEU    </span>|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  30   | tensor(1.1… | 0.56948430… | 0.49168430… | 0.32107311… | 0.47878965… | 50.69493337… | 0.354248958…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  40   | tensor(1.1… | 0.54875436… | 0.50041310… | 0.36202872… | 0.49129506… | 54.16300585… | 0.377904327…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   Loss    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Loss \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_1  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_L  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    SARI    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    BLEU    \u001b[0m|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  30   | tensor(1.1… | 0.56948430… | 0.49168430… | 0.32107311… | 0.47878965… | 50.69493337… | 0.354248958…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  40   | tensor(1.1… | 0.54875436… | 0.50041310… | 0.36202872… | 0.49129506… | 54.16300585… | 0.377904327…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">    Loss     </span>|<span style=\"font-weight: bold\"> ROUGE_Loss  </span>|<span style=\"font-weight: bold\">   ROUGE_1   </span>|<span style=\"font-weight: bold\">   ROUGE_2   </span>|<span style=\"font-weight: bold\">   ROUGE_L   </span>|<span style=\"font-weight: bold\">     SARI     </span>|<span style=\"font-weight: bold\">     BLEU    </span>|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  30   | tensor(1.1… | 0.56948430… | 0.49168430… | 0.32107311… | 0.47878965… | 50.69493337… | 0.354248958…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  40   | tensor(1.1… | 0.54875436… | 0.50041310… | 0.36202872… | 0.49129506… | 54.16300585… | 0.377904327…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  50   | tensor(0.9… | 0.60988121… | 0.43929239… | 0.30687362… | 0.42419035… | 53.51978799… | 0.307775161…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   Loss    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Loss \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_1  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_L  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    SARI    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    BLEU    \u001b[0m|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  30   | tensor(1.1… | 0.56948430… | 0.49168430… | 0.32107311… | 0.47878965… | 50.69493337… | 0.354248958…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  40   | tensor(1.1… | 0.54875436… | 0.50041310… | 0.36202872… | 0.49129506… | 54.16300585… | 0.377904327…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  50   | tensor(0.9… | 0.60988121… | 0.43929239… | 0.30687362… | 0.42419035… | 53.51978799… | 0.307775161…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Training Status                                                  </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">    Loss     </span>|<span style=\"font-weight: bold\"> ROUGE_Loss  </span>|<span style=\"font-weight: bold\">   ROUGE_1   </span>|<span style=\"font-weight: bold\">   ROUGE_2   </span>|<span style=\"font-weight: bold\">   ROUGE_L   </span>|<span style=\"font-weight: bold\">     SARI     </span>|<span style=\"font-weight: bold\">     BLEU    </span>|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  30   | tensor(1.1… | 0.56948430… | 0.49168430… | 0.32107311… | 0.47878965… | 50.69493337… | 0.354248958…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  40   | tensor(1.1… | 0.54875436… | 0.50041310… | 0.36202872… | 0.49129506… | 54.16300585… | 0.377904327…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  50   | tensor(0.9… | 0.60988121… | 0.43929239… | 0.30687362… | 0.42419035… | 53.51978799… | 0.307775161…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "|  0   |  60   | tensor(1.5… | 0.56046716… | 0.49574125… | 0.34344332… | 0.47941391… | 51.79185953… | 0.383516774…|\n",
       "|      |       | grad_fn=&lt;N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Training Status                                                  \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   Loss    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mROUGE_Loss \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_1  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_2  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_L  \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    SARI    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    BLEU    \u001b[0m|\n",
       "|------+-------+-------------+-------------+-------------+-------------+-------------+--------------+-------------|\n",
       "|  0   |   0   | tensor(2.1… | 0.52933967… | 0.51444853… | 0.38699014… | 0.51054228… | 53.78802749… | 0.425826081…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  10   | tensor(1.3… | 0.62586853… | 0.42245963… | 0.29169263… | 0.40824211… | 55.14355839… | 0.294070323…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  20   | tensor(1.3… | 0.57972289… | 0.46636126… | 0.32810878… | 0.46636126… | 55.23409437… | 0.293084442…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  30   | tensor(1.1… | 0.56948430… | 0.49168430… | 0.32107311… | 0.47878965… | 50.69493337… | 0.354248958…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  40   | tensor(1.1… | 0.54875436… | 0.50041310… | 0.36202872… | 0.49129506… | 54.16300585… | 0.377904327…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  50   | tensor(0.9… | 0.60988121… | 0.43929239… | 0.30687362… | 0.42419035… | 53.51978799… | 0.307775161…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "|  0   |  60   | tensor(1.5… | 0.56046716… | 0.49574125… | 0.34344332… | 0.47941391… | 51.79185953… | 0.383516774…|\n",
       "|      |       | grad_fn=<N… |             |             |             |             |              |             |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                               Training Epoch Status                                               </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\">  Train_Loss  </span>|<span style=\"font-weight: bold\">  Valid_Loss  </span>|<span style=\"font-weight: bold\">   ROUGE_1    </span>|<span style=\"font-weight: bold\">   ROUGE_2    </span>|<span style=\"font-weight: bold\">    ROUGE_L    </span>|<span style=\"font-weight: bold\">     SARI     </span>|<span style=\"font-weight: bold\">      BLEU     </span>|\n",
       "|------+--------------+--------------+--------------+--------------+---------------+--------------+---------------|\n",
       "|  0   | tensor(1.48… | tensor(1.29… | 0.469237191… | 0.311362227… | 0.4520826321… | 51.84726098… | 0.35215151439…|\n",
       "|      | grad_fn=&lt;Di… |              |              |              |               |              |               |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                               Training Epoch Status                                               \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m Train_Loss \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m Valid_Loss \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_1   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m  ROUGE_2   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m   ROUGE_L   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m    SARI    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m     BLEU     \u001b[0m|\n",
       "|------+--------------+--------------+--------------+--------------+---------------+--------------+---------------|\n",
       "|  0   | tensor(1.48… | tensor(1.29… | 0.469237191… | 0.311362227… | 0.4520826321… | 51.84726098… | 0.35215151439…|\n",
       "|      | grad_fn=<Di… |              |              |              |               |              |               |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07:57:56] </span><span style=\"font-weight: bold\">[</span>Saving Model<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-deeb963aca67&gt;:90</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07:57:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mSaving Model\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                     \u001b[2m<ipython-input-27-deeb963aca67>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m90\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span> Model saved @ Bart_outputs\\model_files\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m Model saved @ Bart_outputs\\model_files\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Validation<span style=\"font-weight: bold\">]</span> Generation on Validation data saved @ Bart_outputs\\valid_predictions.csv\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mValidation\u001b[1m]\u001b[0m Generation on Validation data saved @ Bart_outputs\\valid_predictions.csv\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Logs<span style=\"font-weight: bold\">]</span> Logs saved @ Bart_outputs\\logs.txt\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mLogs\u001b[1m]\u001b[0m Logs saved @ Bart_outputs\\logs.txt\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenzier for encoding the text\n",
    "model_params = {\n",
    "    \"MODEL\": 'facebook/bart-large-cnn',  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TEST_BATCH_SIZE\": 8,  # test batch size\n",
    "    \"TRAIN_EPOCHS\": 1,  # number of training epochs\n",
    "    \"LEARNING_RATE\": 1e-5,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 100,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 75,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "    \"NUM_BEAMS\":4,\n",
    "    \"REPETITION_PENALTY\":2.5, \n",
    "    \"LENGTH_PENALTY\":0.75, \n",
    "}\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# 加载BART模型和分词器\n",
    "model = BartForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "tokenizer = BartTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "model.model.decoder.generation_mode=False\n",
    "\n",
    "training_logger = training_logger_init()\n",
    "epoch_training_logger=epoch_training_logger_init()\n",
    "\n",
    "train_data=df.copy()\n",
    "train_data[\"source_snt\"] = tokenizer.bos_token + train_data[\"source_snt\"] + tokenizer.eos_token\n",
    "ModelTrainer(\n",
    "    dataframe=train_data,\n",
    "    source_text=\"source_snt\",\n",
    "    target_text=\"simplified_snt\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_params=model_params,\n",
    "    output_dir=\"Bart_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb02813",
   "metadata": {},
   "source": [
    "### Bart Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model,tokenizer=load_model(\"Bart_outputs\")\n",
    "model,tokenizer=load_model(\"Bart_outputs\",BartForConditionalGeneration,BartTokenizer,'facebook/bart-large-cnn')\n",
    "test(test_small,model,tokenizer,'small',\"Bart_outputs\")\n",
    "test(test_medium,model,tokenizer,'medium',\"Bart_outputs\")\n",
    "test(test_medium,model,tokenizer,'large',\"Bart_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c96b81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research. It is an important field of study because drones can be automated or robotically controlled. However, in the industrial and academic studies, people with human form of control are not allowed to drive autonomous vehicles.\n"
     ]
    }
   ],
   "source": [
    "model,tokenizer=load_model(\"Bart_outputs\",BartForConditionalGeneration,BartTokenizer,'facebook/bart-large-cnn')\n",
    "text='<s>In the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research, related to drones.</s>'\n",
    "generate_summary(text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d0cd0",
   "metadata": {},
   "source": [
    "## FlaxT5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204412c",
   "metadata": {},
   "source": [
    "### Flax T5 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "436a5488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10:51:48] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading gpt2<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-deeb963aca67&gt;:17</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[10:51:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading gpt2\u001b[33m...\u001b[0m                                              \u001b[2m<ipython-input-27-deeb963aca67>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m17\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-deeb963aca67&gt;:21</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                               \u001b[2m<ipython-input-27-deeb963aca67>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m21\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Sample Data                                                    </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">                      source_text                       </span>|<span style=\"font-weight: bold\">                       target_text                      </span>|\n",
       "|--------------------------------------------------------+--------------------------------------------------------|\n",
       "|     In the modern era of automation and robotics,      | Current academic and industrial research is interested |\n",
       "|autonomous vehicles are currently the focus of academic |                 in autonomous vehicles.                |\n",
       "|      and industrial research, related to drones.       |                                                        |\n",
       "|  With the ever increasing number of unmanned aerial    |    Drones are increasingly used in the civilian and    |\n",
       "|vehicles getting involved in activities in the civilian |      commercial domain and need to be autonomous.      |\n",
       "| and commercial domain, there is an increased need for  |                                                        |\n",
       "|   autonomy in these systems too, related to drones.    |                                                        |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                    Sample Data                                                    \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1m                      source_text                      \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                      target_text                      \u001b[0m|\n",
       "|--------------------------------------------------------+--------------------------------------------------------|\n",
       "|     In the modern era of automation and robotics,      | Current academic and industrial research is interested |\n",
       "|autonomous vehicles are currently the focus of academic |                 in autonomous vehicles.                |\n",
       "|      and industrial research, related to drones.       |                                                        |\n",
       "|  With the ever increasing number of unmanned aerial    |    Drones are increasingly used in the civilian and    |\n",
       "|vehicles getting involved in activities in the civilian |      commercial domain and need to be autonomous.      |\n",
       "| and commercial domain, there is an increased need for  |                                                        |\n",
       "|   autonomy in these systems too, related to drones.    |                                                        |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">648</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m648\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">518</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m518\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">130</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m130\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-deeb963aca67&gt;:79</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                           \u001b[2m<ipython-input-27-deeb963aca67>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m79\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'decoder_input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-47bb78bff2d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m ModelTrainer(\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mdataframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0msource_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"source_snt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-deeb963aca67>\u001b[0m in \u001b[0;36mModelTrainer\u001b[1;34m(dataframe, source_text, target_text, model, tokenizer, model_params, output_dir)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TRAIN_EPOCHS\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactuals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_rouge1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage_rouge2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage_rougeL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage_bleu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage_sari\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mepoch_training_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_rouge1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_rouge2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_rougeL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_sari\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_bleu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-829d9251c578>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, tokenizer, model, device, loader, optimizer, model_params)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0msource_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"source_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         outputs = model(\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'decoder_input_ids'"
     ]
    }
   ],
   "source": [
    "# tokenzier for encoding the text\n",
    "model_params = {\n",
    "    \"MODEL\": 'google/t5-v1-1.1.0',  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TEST_BATCH_SIZE\": 8,  # test batch size\n",
    "    \"TRAIN_EPOCHS\": 1,  # number of training epochs\n",
    "    \"LEARNING_RATE\": 3e-5,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 100,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 75,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "    \"NUM_BEAMS\":4,\n",
    "    \"REPETITION_PENALTY\":2.5, \n",
    "    \"LENGTH_PENALTY\":0.75, \n",
    "}\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 加载预训练的GPT-2模型和tokenizer\n",
    "from transformers import FlaxT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = FlaxT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "training_logger = training_logger_init()\n",
    "epoch_training_logger=epoch_training_logger_init()\n",
    "\n",
    "\n",
    "ModelTrainer(\n",
    "    dataframe=train_data,\n",
    "    source_text=\"source_snt\",\n",
    "    target_text=\"simplified_snt\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_params=model_params,\n",
    "    output_dir=\"GPT2_outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e082d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
